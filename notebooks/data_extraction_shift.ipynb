{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jan  9 08:48:56 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.113.01             Driver Version: 535.113.01   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla V100-SXM2-32GB           Off | 00000000:06:00.0 Off |                    0 |\n",
      "| N/A   31C    P0              43W / 300W |      3MiB / 32768MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2-32GB           Off | 00000000:07:00.0 Off |                    0 |\n",
      "| N/A   56C    P0             235W / 300W |  12419MiB / 32768MiB |     86%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-SXM2-32GB           Off | 00000000:0A:00.0 Off |                    0 |\n",
      "| N/A   60C    P0             247W / 300W |  10765MiB / 32768MiB |     85%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-SXM2-32GB           Off | 00000000:0B:00.0 Off |                    0 |\n",
      "| N/A   50C    P0             185W / 300W |   5467MiB / 32768MiB |     93%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   4  Tesla V100-SXM2-32GB           Off | 00000000:85:00.0 Off |                    0 |\n",
      "| N/A   53C    P0             222W / 300W |   4911MiB / 32768MiB |     83%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   5  Tesla V100-SXM2-32GB           Off | 00000000:86:00.0 Off |                    0 |\n",
      "| N/A   38C    P0              42W / 300W |      3MiB / 32768MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   6  Tesla V100-SXM2-32GB           Off | 00000000:89:00.0 Off |                    0 |\n",
      "| N/A   62C    P0             237W / 300W |   4093MiB / 32768MiB |     70%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   7  Tesla V100-SXM2-32GB           Off | 00000000:8A:00.0 Off |                    0 |\n",
      "| N/A   54C    P0             251W / 300W |   4119MiB / 32768MiB |     68%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    1   N/A  N/A   2695059      C   python                                    12416MiB |\n",
      "|    2   N/A  N/A   2695059      C   python                                    10762MiB |\n",
      "|    3   N/A  N/A   3597572      C   python                                     5462MiB |\n",
      "|    4   N/A  N/A   3597572      C   python                                     4906MiB |\n",
      "|    6   N/A  N/A    251306      C   python                                     4090MiB |\n",
      "|    7   N/A  N/A    251306      C   python                                     4116MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sur06423/miniconda3/envs/vi_trans/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU device: Tesla V100-SXM2-32GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print('GPU device:',torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('No GPU avaialable, Using CPU')\n",
    "\n",
    "torch.cuda.set_device(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def format_time(seconds):\n",
    "    \"\"\"Converts time in seconds to hours, minutes, and seconds format.\"\"\"\n",
    "    hours = seconds // 3600\n",
    "    minutes = (seconds % 3600) // 60\n",
    "    seconds = seconds % 60\n",
    "    return f\"{int(hours)} hours, {int(minutes)} minutes, {int(seconds)} seconds\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for extracting validation frames of split_1: 2262.9929809570312 seconds\n",
      "Time taken for extracting validation frames of split_1: 0 hours, 37 minutes, 42 seconds\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import csv\n",
    "import time\n",
    "\n",
    "class FrameExtractor:\n",
    "    def __init__(self, data_dir, output_dir):\n",
    "        self.data_dir = data_dir\n",
    "        self.output_dir = output_dir\n",
    "\n",
    "    def extract_frames(self, row, max_frames_per_chunk):\n",
    "        participant_id, file_id, annotation_id, frame_start, frame_end, activity, chunk_id = row\n",
    "        video_filepath = os.path.join(self.data_dir, file_id + '.mp4')\n",
    "        new_file_id = file_id.replace(\"/\", \"_\")\n",
    "        updated_output_dir = os.path.join(self.output_dir, f'{activity}', f'{participant_id}_{new_file_id}_frames_{frame_start}_{frame_end}_ann_{annotation_id}_chunk_{chunk_id}')\n",
    "        \n",
    "        os.makedirs(updated_output_dir, exist_ok=True)\n",
    "        cap = cv2.VideoCapture(video_filepath)\n",
    "        frame_count = 0\n",
    "         \n",
    "        try:\n",
    "            # Set the starting frame position\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, int(frame_start))\n",
    "            \n",
    "            for frame_num in range(int(frame_start), int(frame_end) + 1):\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    print(f\"Frame number {frame_num} is missing.\")\n",
    "                    break\n",
    "                #new_file_id = file_id.replace(\"/\", \"_\")\n",
    "                output_filename = f'img_{frame_num:06d}.png'\n",
    "                output_path = os.path.join(updated_output_dir, output_filename)\n",
    "                cv2.imwrite(output_path, frame)\n",
    "                frame_count += 1\n",
    "                \n",
    "                if frame_count > max_frames_per_chunk:\n",
    "                    break\n",
    "        finally:\n",
    "            cap.release()\n",
    "\n",
    "def process_annotations(annotation_file, data_dir, root_dataset_dir, dataset_sub_dir, max_frames_per_chunk=48):\n",
    "    with open(annotation_file, 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        next(reader)  # Skip header row\n",
    "        output_dir = os.path.join(root_dataset_dir, dataset_sub_dir)\n",
    "        frame_extractor = FrameExtractor(data_dir, output_dir)\n",
    "        \n",
    "        for row in reader:\n",
    "            frame_extractor.extract_frames(row, max_frames_per_chunk)\n",
    "\n",
    "# First : Train : Split_1, \n",
    "def main():\n",
    "    data_dir = \"/home/sur06423/hiwi/vit_exp/vision_tranformer_baseline/data/kinect_color/kinect_color\"\n",
    "    # \"/home/sur06423/hiwi/vit_exp/vision_tranformer_baseline/daa_dataset\"\n",
    "    root_dataset_dir = \"/net/polaris/storage/deeplearning/sur_data/rgb_daa/split_1\"\n",
    "    dataset_sub_dirs = ['val'] #, 'train', 'test', 'val'\n",
    "    annotation_files = [\n",
    "        '/home/sur06423/hiwi/vit_exp/vision_tranformer_baseline/data/kinect_color_annotation/activities_3s/kinect_color/midlevel.chunks_90.split_1.val.csv'\n",
    "    ] #,\n",
    "      #  '/home/sur06423/hiwi/vit_exp/vision_tranformer_baseline/data/kinect_color_annotation/activities_3s/kinect_color/midlevel.chunks_90.split_0.train.csv',\n",
    "      #  '/home/sur06423/hiwi/vit_exp/vision_tranformer_baseline/data/kinect_color_annotation/activities_3s/kinect_color/midlevel.chunks_90.split_0.test.csv'\n",
    "    \n",
    "    # # Start timing before the loop\n",
    "    start_time = time.time()\n",
    "\n",
    "    for annotation_file, dataset_sub_dir in zip(annotation_files, dataset_sub_dirs):\n",
    "        process_annotations(annotation_file, data_dir, root_dataset_dir, dataset_sub_dir)\n",
    "    \n",
    "    end_time = time.time()  # End timing after the loop\n",
    "    time_taken = end_time - start_time  # Calculating time taken\n",
    "    print(f\"Time taken for extracting validation frames of split_1: {time_taken} seconds\")\n",
    "    formatted_time = format_time(time_taken)  # Convert to hours, minutes, seconds format\n",
    "    print(f\"Time taken for extracting validation frames of split_1: {formatted_time}\")\n",
    "\n",
    "#if __name__ == \"__main__\":\n",
    "#    main()\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vi_trans",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
