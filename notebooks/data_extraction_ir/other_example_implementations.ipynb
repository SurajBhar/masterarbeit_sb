{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel Processing Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import math\n",
    "\n",
    "PRIMES = [\n",
    "    112272535095293,\n",
    "    112582705942171,\n",
    "    112272535095293,\n",
    "    115280095190773,\n",
    "    115797848077099,\n",
    "    1099726899285419]\n",
    "\n",
    "def is_prime(n):\n",
    "    if n < 2:\n",
    "        return False\n",
    "    if n == 2:\n",
    "        return True\n",
    "    if n % 2 == 0:\n",
    "        return False\n",
    "\n",
    "    sqrt_n = int(math.floor(math.sqrt(n)))\n",
    "    for i in range(3, sqrt_n + 1, 2):\n",
    "        if n % i == 0:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    It creates a ProcessPoolExecutor context manager, which will manage a pool of worker processes.\n",
    "    executor.map(is_prime, PRIMES) maps the is_prime function over the PRIMES list, distributing the work across multiple processes. \n",
    "    This allows for the prime checks to be executed in parallel, potentially speeding up the computation on multi-core processors.\n",
    "    It uses zip to iterate over the PRIMES list and the results of executor.map simultaneously, \n",
    "    printing whether each number is prime.\n",
    "    \"\"\"\n",
    "    with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "        for number, prime in zip(PRIMES, executor.map(is_prime, PRIMES)):\n",
    "            print('%d is prime: %s' % (number, prime))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import math\n",
    "\n",
    "PRIMES = [\n",
    "    112272535095293,\n",
    "    112582705942171,\n",
    "    112272535095293,\n",
    "    115280095190773,\n",
    "    115797848077099,\n",
    "    1099726899285419]\n",
    "\n",
    "def is_prime(n):\n",
    "    if n < 2:\n",
    "        return False\n",
    "    if n == 2:\n",
    "        return True\n",
    "    if n % 2 == 0:\n",
    "        return False\n",
    "    \n",
    "    sqrt_n = int(math.floor(math.sqrt(n)))\n",
    "    for i in range(3, sqrt_n + 1, 2):\n",
    "        if n % i == 0:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def main():\n",
    "    with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "        # Using submit() to start is_prime function for each number in PRIMES\n",
    "        futures = [executor.submit(is_prime, num) for num in PRIMES]\n",
    "        \n",
    "        for num, future in zip(PRIMES, futures):\n",
    "            print('%d is prime: %s' % (num, future.result()))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import math\n",
    "\n",
    "names = [\"suraj\", \"ankush\", \"david\",\"bastian\"]\n",
    "\n",
    "def is_suraj(name):\n",
    "    if name == \"suraj\":\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def main():\n",
    "    with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "        # Using submit() to start is_prime function for each number in PRIMES\n",
    "        futures = [executor.submit(is_suraj, name) for name in names]\n",
    "        \n",
    "        for name, future in zip(names, futures):\n",
    "            print(f\"{name} is: {future.result()}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import csv\n",
    "import time\n",
    "import concurrent.futures\n",
    "\n",
    "def format_time(seconds):\n",
    "    \"\"\"Converts time in seconds to hours, minutes, and seconds format.\"\"\"\n",
    "    hours = seconds // 3600\n",
    "    minutes = (seconds % 3600) // 60\n",
    "    seconds = seconds % 60\n",
    "    return f\"{int(hours)} hours, {int(minutes)} minutes, {int(seconds)} seconds\"\n",
    "\n",
    "def extract_frames(data_dir, output_dir, row, max_frames_per_chunk):\n",
    "    try:\n",
    "        participant_id, file_id, annotation_id, frame_start, frame_end, activity, chunk_id = row\n",
    "        video_filepath = os.path.join(data_dir, file_id + '.mp4')\n",
    "        new_file_id = file_id.replace(\"/\", \"_\")\n",
    "        updated_output_dir = os.path.join(output_dir, f'{activity}', f'{participant_id}_{new_file_id}_frames_{frame_start}_{frame_end}_ann_{annotation_id}_chunk_{chunk_id}')\n",
    "        \n",
    "        os.makedirs(updated_output_dir, exist_ok=True)\n",
    "        cap = cv2.VideoCapture(video_filepath)\n",
    "        frame_count = 0\n",
    "         \n",
    "        # Set the starting frame position\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, int(frame_start))\n",
    "        \n",
    "        for frame_num in range(int(frame_start), int(frame_end) + 1):\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                print(f\"Frame number {frame_num} is missing.\")\n",
    "                break\n",
    "            output_filename = f'img_{frame_num:06d}.png'\n",
    "            output_path = os.path.join(updated_output_dir, output_filename)\n",
    "            cv2.imwrite(output_path, frame)\n",
    "            frame_count += 1\n",
    "            \n",
    "            if frame_count > max_frames_per_chunk:\n",
    "                break\n",
    "\n",
    "        cap.release()\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_id}: {e}\")\n",
    "\n",
    "def process_annotations_parallel(annotation_file, data_dir, root_dataset_dir, dataset_sub_dir, max_frames_per_chunk=48):\n",
    "    try:\n",
    "        with open(annotation_file, 'r') as csvfile:\n",
    "            reader = csv.reader(csvfile)\n",
    "            next(reader)  # Skip header row\n",
    "            output_dir = os.path.join(root_dataset_dir, dataset_sub_dir)\n",
    "            \n",
    "            with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "                futures = [executor.submit(extract_frames, data_dir, output_dir, row, max_frames_per_chunk) for row in reader]\n",
    "                concurrent.futures.wait(futures)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading annotation file: {e}\")\n",
    "\n",
    "def main():\n",
    "    data_dir = \"/home/sur06423/hiwi/vit_exp/vision_tranformer_baseline/data/kinect_ir\"\n",
    "    root_dataset_dir = \"/net/polaris/storage/deeplearning/sur_data/kinect_ir_daa/split_0\"\n",
    "    dataset_sub_dirs = ['train', 'test', 'val']\n",
    "    annotation_files = [\n",
    "        \"/home/sur06423/hiwi/vit_exp/vision_tranformer_baseline/data/kinect_color_annotation/activities_3s/kinect_ir/midlevel.chunks_90.split_0.test.csv\",\n",
    "        \"/home/sur06423/hiwi/vit_exp/vision_tranformer_baseline/data/kinect_color_annotation/activities_3s/kinect_ir/midlevel.chunks_90.split_0.train.csv\",\n",
    "        \"/home/sur06423/hiwi/vit_exp/vision_tranformer_baseline/data/kinect_color_annotation/activities_3s/kinect_ir/midlevel.chunks_90.split_0.val.csv\"\n",
    "    ]\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for annotation_file, dataset_sub_dir in zip(annotation_files, dataset_sub_dirs):\n",
    "        process_annotations_parallel(annotation_file, data_dir, root_dataset_dir, dataset_sub_dir)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    time_taken = end_time - start_time\n",
    "    print(f\"Time taken for extracting dataset frames of split_0 of kinect_ir_right_top view: {format_time(time_taken)}\")\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "These implementations are not used for the extraction of the frames but they could be modified for further developments or use cases.\n",
    "\"\"\"\n",
    "import os\n",
    "import cv2\n",
    "import csv\n",
    "import time\n",
    "import concurrent.futures\n",
    "\n",
    "class FrameExtractor:\n",
    "    \"\"\"Extract frames from video files based on annotations.\n",
    "\n",
    "    Attributes:\n",
    "        data_dir (str): Directory where video files are stored.\n",
    "        root_dataset_dir (str): Root directory for storing extracted frames.\n",
    "        max_frames_per_chunk (int): Maximum number of frames to extract per chunk.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_dir, root_dataset_dir, max_frames_per_chunk=48):\n",
    "        self.data_dir = data_dir\n",
    "        self.root_dataset_dir = root_dataset_dir\n",
    "        self.max_frames_per_chunk = max_frames_per_chunk\n",
    "\n",
    "    def _extract_frames(self, output_dir, row):\n",
    "        \"\"\"Extract frames for a single video file based on annotation row.\"\"\"\n",
    "        try:\n",
    "            participant_id, file_id, annotation_id, frame_start, frame_end, activity, chunk_id = row\n",
    "            video_filepath = os.path.join(self.data_dir, file_id + '.mp4')\n",
    "            new_file_id = file_id.replace(\"/\", \"_\")\n",
    "            updated_output_dir = os.path.join(output_dir, f'{activity}', f'{participant_id}_{new_file_id}_frames_{frame_start}_{frame_end}_ann_{annotation_id}_chunk_{chunk_id}')\n",
    "            \n",
    "            os.makedirs(updated_output_dir, exist_ok=True)\n",
    "            cap = cv2.VideoCapture(video_filepath)\n",
    "            frame_count = 0\n",
    "             \n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, int(frame_start))\n",
    "            \n",
    "            for frame_num in range(int(frame_start), int(frame_end) + 1):\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    print(f\"Frame number {frame_num} is missing.\")\n",
    "                    break\n",
    "                output_filename = f'img_{frame_num:06d}.png'\n",
    "                output_path = os.path.join(updated_output_dir, output_filename)\n",
    "                cv2.imwrite(output_path, frame)\n",
    "                frame_count += 1\n",
    "                \n",
    "                if frame_count > self.max_frames_per_chunk:\n",
    "                    break\n",
    "\n",
    "            cap.release()\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_id}: {e}\")\n",
    "\n",
    "    def process_annotations_parallel(self, annotation_file, dataset_sub_dir):\n",
    "        \"\"\"Process annotations in parallel, extracting frames for each video.\"\"\"\n",
    "        try:\n",
    "            with open(annotation_file, 'r') as csvfile:\n",
    "                reader = csv.reader(csvfile)\n",
    "                next(reader)  # Skip header row\n",
    "                output_dir = os.path.join(self.root_dataset_dir, dataset_sub_dir)\n",
    "                \n",
    "                with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "                    futures = [executor.submit(self._extract_frames, output_dir, row) for row in reader]\n",
    "                    concurrent.futures.wait(futures)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading annotation file: {e}\")\n",
    "\n",
    "def format_time(seconds):\n",
    "    \"\"\"Converts time in seconds to hours, minutes, and seconds format.\"\"\"\n",
    "    hours = seconds // 3600\n",
    "    minutes = (seconds % 3600) // 60\n",
    "    seconds = seconds % 60\n",
    "    return f\"{int(hours)} hours, {int(minutes)} minutes, {int(seconds)} seconds\"\n",
    "\n",
    "def main():\n",
    "    data_dir = \"/home/sur06423/hiwi/vit_exp/vision_tranformer_baseline/data/kinect_ir\"\n",
    "    root_dataset_dir = \"/net/polaris/storage/deeplearning/sur_data/kinect_ir_daa/split_0\"\n",
    "    dataset_sub_dirs = ['train', 'test', 'val']\n",
    "    annotation_files = [\n",
    "        \"/home/sur06423/hiwi/vit_exp/vision_tranformer_baseline/data/kinect_color_annotation/activities_3s/kinect_ir/midlevel.chunks_90.split_0.test.csv\",\n",
    "        \"/home/sur06423/hiwi/vit_exp/vision_tranformer_baseline/data/kinect_color_annotation/activities_3s/kinect_ir/midlevel.chunks_90.split_0.train.csv\",\n",
    "        \"/home/sur06423/hiwi/vit_exp/vision_tranformer_baseline/data/kinect_color_annotation/activities_3s/kinect_ir/midlevel.chunks_90.split_0.val.csv\"\n",
    "    ]\n",
    "\n",
    "    extractor = FrameExtractor(data_dir, root_dataset_dir)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for annotation_file, dataset_sub_dir in zip(annotation_files, dataset_sub_dirs):\n",
    "        extractor.process_annotations_parallel(annotation_file, dataset_sub_dir)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    time_taken = end_time - start_time\n",
    "    print(f\"Time taken for extracting dataset frames of split_0 of kinect_ir_right_top view: {format_time(time_taken)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two class OOP structure separting Annotation processing and Frame Extraction Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import csv\n",
    "import time\n",
    "import concurrent.futures\n",
    "\n",
    "class FrameExtractor:\n",
    "    \"\"\"\n",
    "    A class to extract frames from video files based on annotations.\n",
    "    \n",
    "    Attributes:\n",
    "        data_dir (str): The directory where the video files are stored.\n",
    "        output_dir (str): The root directory where the extracted frames will be saved.\n",
    "        max_frames_per_chunk (int): The maximum number of frames to extract per chunk.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir, output_dir, max_frames_per_chunk=48):\n",
    "        \"\"\"\n",
    "        Initializes the FrameExtractor with directories and extraction parameters.\n",
    "        \"\"\"\n",
    "        self.data_dir = data_dir\n",
    "        self.output_dir = output_dir\n",
    "        self.max_frames_per_chunk = max_frames_per_chunk\n",
    "    \n",
    "    def extract_frames(self, row):\n",
    "        \"\"\"\n",
    "        Extracts frames from a video file based on a row from the annotation file.\n",
    "        \n",
    "        Args:\n",
    "            row (list): A list containing annotation information for a video segment.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            participant_id, file_id, annotation_id, frame_start, frame_end, activity, chunk_id = row\n",
    "            video_filepath = os.path.join(self.data_dir, file_id + '.mp4')\n",
    "            new_file_id = file_id.replace(\"/\", \"_\")\n",
    "            updated_output_dir = os.path.join(self.output_dir, f'{activity}', f'{participant_id}_{new_file_id}_frames_{frame_start}_{frame_end}_ann_{annotation_id}_chunk_{chunk_id}')\n",
    "            \n",
    "            os.makedirs(updated_output_dir, exist_ok=True)\n",
    "            cap = cv2.VideoCapture(video_filepath)\n",
    "            frame_count = 0\n",
    "             \n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, int(frame_start))\n",
    "            \n",
    "            for frame_num in range(int(frame_start), int(frame_end) + 1):\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    print(f\"Frame number {frame_num} is missing.\")\n",
    "                    break\n",
    "                output_filename = f'img_{frame_num:06d}.png'\n",
    "                output_path = os.path.join(updated_output_dir, output_filename)\n",
    "                cv2.imwrite(output_path, frame)\n",
    "                frame_count += 1\n",
    "                \n",
    "                if frame_count > self.max_frames_per_chunk:\n",
    "                    break\n",
    "\n",
    "            cap.release()\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_id}: {e}\")\n",
    "\n",
    "class AnnotationProcessor:\n",
    "    \"\"\"\n",
    "    A class to process annotation files and extract frames from videos in parallel.\n",
    "    \n",
    "    Attributes:\n",
    "        annotation_files (list): A list of paths to the annotation files.\n",
    "        data_dir (str): The directory where the video files are stored.\n",
    "        root_dataset_dir (str): The root directory where the extracted frames will be saved.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, annotation_files, data_dir, root_dataset_dir):\n",
    "        \"\"\"\n",
    "        Initializes the AnnotationProcessor with the dataset and annotation information.\n",
    "        \"\"\"\n",
    "        self.annotation_files = annotation_files\n",
    "        self.data_dir = data_dir\n",
    "        self.root_dataset_dir = root_dataset_dir\n",
    "    \n",
    "    def process_annotations(self, dataset_sub_dir, max_frames_per_chunk):\n",
    "        \"\"\"\n",
    "        Processes a single annotation file in parallel using multiple processes.\n",
    "        \n",
    "        Args:\n",
    "            dataset_sub_dir (str): The sub-directory (e.g., 'train', 'test') for saving extracted frames.\n",
    "            max_frames_per_chunk (int): The maximum number of frames to extract per chunk.\n",
    "        \"\"\"\n",
    "        annotation_file = self.annotation_files[dataset_sub_dir]\n",
    "        output_dir = os.path.join(self.root_dataset_dir, dataset_sub_dir)\n",
    "        frame_extractor = FrameExtractor(self.data_dir, output_dir, max_frames_per_chunk)\n",
    "        \n",
    "        try:\n",
    "            with open(annotation_file, 'r') as csvfile:\n",
    "                reader = csv.reader(csvfile)\n",
    "                next(reader)  # Skip header row\n",
    "                \n",
    "                with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "                    futures = [executor.submit(frame_extractor.extract_frames, row) for row in reader]\n",
    "                    concurrent.futures.wait(futures)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing annotation file {annotation_file}: {e}\")\n",
    "\n",
    "def format_time(seconds):\n",
    "    \"\"\"Converts time in seconds to hours, minutes, and seconds format.\"\"\"\n",
    "    hours = seconds // 3600\n",
    "    minutes = (seconds % 3600) // 60\n",
    "    seconds = seconds % 60\n",
    "    return f\"{int(hours)} hours, {int(minutes)} minutes, {int(seconds)} seconds\"\n",
    "\n",
    "def main():\n",
    "    data_dir = \"/home/sur06423/hiwi/vit_exp/vision_tranformer_baseline/data/kinect_ir\"\n",
    "    root_dataset_dir = \"/net/polaris/storage/deeplearning/sur_data/kinect_ir_daa/split_0\"\n",
    "    dataset_sub_dirs = ['train', 'test', 'val']\n",
    "    annotation_files = {\n",
    "        'test': \"/home/sur06423/hiwi/vit_exp/vision_tranformer_baseline/data/kinect_color_annotation/activities_3s/kinect_ir/midlevel.chunks_90.split_0.test.csv\",\n",
    "        'train': \"/home/sur06423/hiwi/vit_exp/vision_tranformer_baseline/data/kinect_color_annotation/activities_3s/kinect_ir/midlevel.chunks_90.split_0.train.csv\",\n",
    "        'val': \"/home/sur06423/hiwi/vit_exp/vision_tranformer_baseline/data/kinect_color_annotation/activities_3s/kinect_ir/midlevel.chunks_90.split_0.val.csv\"\n",
    "    }\n",
    "\n",
    "    annotation_processor = AnnotationProcessor(annotation_files, data_dir, root_dataset_dir)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for dataset_sub_dir in dataset_sub_dirs:\n",
    "        max_frames_per_chunk = 48  # Customize this value as needed\n",
    "        print(f\"Processing {dataset_sub_dir}...\")\n",
    "        annotation_processor.process_annotations(dataset_sub_dir, max_frames_per_chunk)\n",
    "\n",
    "    end_time = time.time()\n",
    "    time_taken = format_time(end_time - start_time)\n",
    "    print(f\"Time taken for extracting dataset frames of split_0 of kinect_ir_right_top view: {time_taken}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import csv\n",
    "import time\n",
    "import concurrent.futures\n",
    "import logging\n",
    "\n",
    "# Set up basic configuration for logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(processName)s - %(levelname)s - %(message)s')\n",
    "\n",
    "class VideoFrameExtractor:\n",
    "    \"\"\"\n",
    "    Extracts frames from video files based on annotations and processes these annotations in parallel.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_dir, root_dataset_dir, annotation_files, num_workers=4):\n",
    "        self.data_dir = data_dir\n",
    "        self.root_dataset_dir = root_dataset_dir\n",
    "        self.annotation_files = annotation_files\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "    def extract_frames(self, row, output_dir, max_frames_per_chunk):\n",
    "        participant_id, file_id, annotation_id, frame_start, frame_end, activity, chunk_id = row\n",
    "        video_filepath = os.path.join(self.data_dir, file_id + '.mp4')\n",
    "        new_file_id = file_id.replace(\"/\", \"_\")\n",
    "        updated_output_dir = os.path.join(output_dir, f'{activity}', f'{participant_id}_{new_file_id}_frames_{frame_start}_{frame_end}_ann_{annotation_id}_chunk_{chunk_id}')\n",
    "        \n",
    "        os.makedirs(updated_output_dir, exist_ok=True)\n",
    "        cap = cv2.VideoCapture(video_filepath)\n",
    "        frame_count = 0\n",
    "         \n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, int(frame_start))\n",
    "        \n",
    "        logging.info(f\"Starting frame extraction for {file_id} by process ID: {os.getpid()}\")\n",
    "        for frame_num in range(int(frame_start), int(frame_end) + 1):\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                logging.warning(f\"Frame number {frame_num} is missing in {file_id}.\")\n",
    "                break\n",
    "            output_filename = f'img_{frame_num:06d}.png'\n",
    "            output_path = os.path.join(updated_output_dir, output_filename)\n",
    "            cv2.imwrite(output_path, frame)\n",
    "            frame_count += 1\n",
    "            \n",
    "            if frame_count > max_frames_per_chunk:\n",
    "                break\n",
    "\n",
    "        cap.release()\n",
    "        logging.info(f\"Completed frame extraction for {file_id}\")\n",
    "\n",
    "    def process_annotations(self, dataset_sub_dir, max_frames_per_chunk):\n",
    "        annotation_file = self.annotation_files[dataset_sub_dir]\n",
    "        output_dir = os.path.join(self.root_dataset_dir, dataset_sub_dir)\n",
    "        \n",
    "        with open(annotation_file, 'r') as csvfile:\n",
    "            reader = csv.reader(csvfile)\n",
    "            next(reader)  # Skip header row\n",
    "            \n",
    "            with concurrent.futures.ProcessPoolExecutor(max_workers=self.num_workers) as executor:\n",
    "                futures = [executor.submit(self.extract_frames, row, output_dir, max_frames_per_chunk) for row in reader]\n",
    "                concurrent.futures.wait(futures)\n",
    "\n",
    "def format_time(seconds):\n",
    "    hours = seconds // 3600\n",
    "    minutes = (seconds % 3600) // 60\n",
    "    seconds = seconds % 60\n",
    "    return f\"{int(hours)}h:{int(minutes)}m:{int(seconds)}s\"\n",
    "\n",
    "def main():\n",
    "    data_dir = \"/path/to/your/video/files\"\n",
    "    root_dataset_dir = \"/path/to/your/output/directory\"\n",
    "    annotation_files = {\n",
    "        'train': \"/path/to/train_annotation_file.csv\",\n",
    "        'test': \"/path/to/test_annotation_file.csv\",\n",
    "        'val': \"/path/to/val_annotation_file.csv\"\n",
    "    }\n",
    "    num_workers = 4  # Adjust based on your system's capabilities\n",
    "\n",
    "    video_frame_extractor = VideoFrameExtractor(data_dir, root_dataset_dir, annotation_files, num_workers)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for dataset_sub_dir in annotation_files.keys():\n",
    "        max_frames_per_chunk = 48\n",
    "        logging.info(f\"Processing {dataset_sub_dir} dataset...\")\n",
    "        video_frame_extractor.process_annotations(dataset_sub_dir, max_frames_per_chunk)\n",
    "\n",
    "    end_time = time.time()\n",
    "    time_taken = format_time(end_time - start_time)\n",
    "    logging.info(f\"Time taken for extracting dataset frames: {time_taken}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
