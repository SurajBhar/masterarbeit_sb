{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All the annotation files and their corresponding paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/home/sur06423/hiwi/vit_exp/vision_tranformer_baseline/data/kinect_ir\"\n",
    "# the output directory where the extracted frames will be stored \n",
    "root_dataset_dir = \"/net/polaris/storage/deeplearning/sur_data/kinect_ir_daa/split_1\"\n",
    "annotation_files = {\n",
    "    'train': \"/home/sur06423/hiwi/vit_exp/vision_tranformer_baseline/data/kinect_color_annotation/activities_3s/kinect_ir/midlevel.chunks_90.split_1.train.csv\",\n",
    "    'test': \"/home/sur06423/hiwi/vit_exp/vision_tranformer_baseline/data/kinect_color_annotation/activities_3s/kinect_ir/midlevel.chunks_90.split_1.test.csv\",\n",
    "    'val': \"/home/sur06423/hiwi/vit_exp/vision_tranformer_baseline/data/kinect_color_annotation/activities_3s/kinect_ir/midlevel.chunks_90.split_1.val.csv\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "kinect_ir_annotation_files_split_0 = [\n",
    "    \"/home/sur06423/hiwi/vit_exp/vision_tranformer_baseline/data/kinect_color_annotation/activities_3s/kinect_ir/midlevel.chunks_90.split_0.test.csv\",\n",
    "    \"/home/sur06423/hiwi/vit_exp/vision_tranformer_baseline/data/kinect_color_annotation/activities_3s/kinect_ir/midlevel.chunks_90.split_0.train.csv\",\n",
    "    \"/home/sur06423/hiwi/vit_exp/vision_tranformer_baseline/data/kinect_color_annotation/activities_3s/kinect_ir/midlevel.chunks_90.split_0.val.csv\"\n",
    "]\n",
    "\n",
    "kinect_ir_annotation_files_split_1 = [\n",
    "    \"/home/sur06423/hiwi/vit_exp/vision_tranformer_baseline/data/kinect_color_annotation/activities_3s/kinect_ir/midlevel.chunks_90.split_1.test.csv\",\n",
    "    \"/home/sur06423/hiwi/vit_exp/vision_tranformer_baseline/data/kinect_color_annotation/activities_3s/kinect_ir/midlevel.chunks_90.split_1.train.csv\",\n",
    "    \"/home/sur06423/hiwi/vit_exp/vision_tranformer_baseline/data/kinect_color_annotation/activities_3s/kinect_ir/midlevel.chunks_90.split_1.val.csv\"\n",
    "]\n",
    "\n",
    "kinect_ir_annotation_files_split_2 = [\n",
    "    \"/home/sur06423/hiwi/vit_exp/vision_tranformer_baseline/data/kinect_color_annotation/activities_3s/kinect_ir/midlevel.chunks_90.split_2.test.csv\",\n",
    "    \"/home/sur06423/hiwi/vit_exp/vision_tranformer_baseline/data/kinect_color_annotation/activities_3s/kinect_ir/midlevel.chunks_90.split_2.train.csv\",\n",
    "    \"/home/sur06423/hiwi/vit_exp/vision_tranformer_baseline/data/kinect_color_annotation/activities_3s/kinect_ir/midlevel.chunks_90.split_2.val.csv\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train /home/sur06423/hiwi/vit_exp/vision_tranformer_baseline/data/kinect_color_annotation/activities_3s/kinect_color/midlevel.chunks_90.split_2.train.csv\n",
      "test /home/sur06423/hiwi/vit_exp/vision_tranformer_baseline/data/kinect_color_annotation/activities_3s/kinect_color/midlevel.chunks_90.split_2.test.csv\n",
      "val /home/sur06423/hiwi/vit_exp/vision_tranformer_baseline/data/kinect_color_annotation/activities_3s/kinect_color/midlevel.chunks_90.split_2.val.csv\n"
     ]
    }
   ],
   "source": [
    "list_1 = [1,2,3]\n",
    "list_2 = ['a','b','c']\n",
    "\n",
    "dataset_sub_dirs = ['train', 'test', 'val']\n",
    "annotation_files = [\n",
    "        '/home/sur06423/hiwi/vit_exp/vision_tranformer_baseline/data/kinect_color_annotation/activities_3s/kinect_color/midlevel.chunks_90.split_2.train.csv',\n",
    "        '/home/sur06423/hiwi/vit_exp/vision_tranformer_baseline/data/kinect_color_annotation/activities_3s/kinect_color/midlevel.chunks_90.split_2.test.csv',\n",
    "        '/home/sur06423/hiwi/vit_exp/vision_tranformer_baseline/data/kinect_color_annotation/activities_3s/kinect_color/midlevel.chunks_90.split_2.val.csv',\n",
    "    ]\n",
    "\n",
    "for x,y in zip(dataset_sub_dirs, annotation_files):\n",
    "    print(x,y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiprocessing Based Frame Extraction from Videos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paths:\n",
    "- /net/polaris/storage/deeplearning/sur_data/kinect_ir_daa\n",
    "- /net/polaris/storage/deeplearning/sur_data/nir_front_top_daa\n",
    "\n",
    "- /home/sur06423/hiwi/vit_exp/vision_tranformer_baseline/data/inner_mirror\n",
    "- /home/sur06423/hiwi/vit_exp/vision_tranformer_baseline/data/kinect_ir"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functionality of the VideoFrameExtractor script can be described as follows:\n",
    "\n",
    "# Overview\n",
    "The script's purpose is to extract specific frames from video files based on annotations provided in CSV files. Each annotation defines which frames to extract from which video file, associated with a particular activity and other metadata. The script processes these annotations in parallel, leveraging multiple CPU cores to speed up the frame extraction process from a potentially large set of video files.\n",
    "\n",
    "# How It Works\n",
    "- Initialization: \n",
    "The script starts by creating an instance of the VideoFrameExtractor class, initializing it with the paths to the video files (data_dir), the output directory where the extracted frames will be stored (root_dataset_dir), and a dictionary mapping dataset splits (like 'train', 'test', 'val') to their respective annotation files.\n",
    "\n",
    "- Processing Annotations: \n",
    "For each dataset split, the script reads the corresponding annotation file. Each row in this file contains details about a specific video file and the range of frames to extract, among other information.\n",
    "\n",
    "- Parallel Processing: \n",
    "To efficiently handle the potentially intensive task of reading video files and extracting frames, the script uses Python's concurrent.futures.ProcessPoolExecutor to execute multiple instances of the frame extraction task in parallel. This is where parallel processing comes into play.\n",
    "\n",
    "- Process Pool Executor: \n",
    "A pool of worker processes is created (the size of the pool can be adjusted but typically matches the number of available CPU cores). Each worker process can execute tasks independently.\n",
    "\n",
    "- Submitting Tasks: \n",
    "For each row in the annotation file (each representing a video file and specific frames to extract), a task is submitted to the process pool. This task involves calling the extract_frames method with the row data and additional parameters like the output directory and the maximum number of frames to extract per chunk.\n",
    "\n",
    "- Execution: \n",
    "The process pool manages a queue of these tasks, assigning them to available worker processes. Since each worker process runs in its own Python interpreter process, they execute in parallel, utilizing multiple CPU cores. This parallel execution significantly speeds up the frame extraction process compared to a sequential approach.\n",
    "\n",
    "- Task Handling: \n",
    "Each worker process executes the extract_frames method independently. This involves reading the specified video file, seeking to the start frame, and then extracting and saving each frame up to the end frame defined in the annotation. The frames are saved to the specified output directory, organized by activity, participant, and other metadata from the annotation.\n",
    "\n",
    "- Completion: \n",
    "Once all tasks have been processed, and all frames have been extracted and saved, the script concludes its execution. The parallel processing ensures that this happens much faster than it would if each video file were processed sequentially, one after the other.\n",
    "\n",
    "- Summary\n",
    "The script's use of parallel processing allows it to efficiently handle large datasets by distributing the workload across multiple CPU cores. This is particularly advantageous for video processing tasks, which are computationally intensive and can take significant time when done sequentially. By using a ProcessPoolExecutor, the script launches parallel processes, each working on extracting frames from different video files as specified by the annotations, thereby significantly reducing the total processing time."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The linux command htop can list all the python processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing train dataset...\n",
      "Processing test dataset...\n",
      "Processing val dataset...\n",
      "Time taken for extracting dataset frames: 0 hours, 17 minutes, 39 seconds\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import csv\n",
    "import time\n",
    "import concurrent.futures\n",
    "\n",
    "class VideoFrameExtractor:\n",
    "    \"\"\"\n",
    "    A class to extract frames from video files based on annotations and process these annotations in parallel.\n",
    "    \n",
    "    Attributes:\n",
    "        data_dir (str): The directory where the video files are stored.\n",
    "        root_dataset_dir (str): The root directory where the extracted frames will be saved.\n",
    "        annotation_files (dict): A dictionary mapping dataset splits to their corresponding annotation file paths.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir, root_dataset_dir, annotation_files):\n",
    "        \"\"\"\n",
    "        Initializes the VideoFrameExtractor with dataset directories and annotation file paths.\n",
    "        \"\"\"\n",
    "        self.data_dir = data_dir\n",
    "        self.root_dataset_dir = root_dataset_dir\n",
    "        self.annotation_files = annotation_files\n",
    "\n",
    "    def extract_frames(self, row, output_dir, max_frames_per_chunk):\n",
    "        \"\"\"\n",
    "        Extracts frames from a video file based on a row from the annotation file.\n",
    "        \n",
    "        Args:\n",
    "            row (list): A list containing annotation information for a video segment.\n",
    "            output_dir (str): The directory where the extracted frames should be saved.\n",
    "            max_frames_per_chunk (int): The maximum number of frames to extract per chunk.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            participant_id, file_id, annotation_id, frame_start, frame_end, activity, chunk_id = row\n",
    "            video_filepath = os.path.join(self.data_dir, file_id + '.mp4')\n",
    "            new_file_id = file_id.replace(\"/\", \"_\")\n",
    "            updated_output_dir = os.path.join(output_dir, f'{activity}', f'{participant_id}_{new_file_id}_frames_{frame_start}_{frame_end}_ann_{annotation_id}_chunk_{chunk_id}')\n",
    "            \n",
    "            os.makedirs(updated_output_dir, exist_ok=True)\n",
    "            cap = cv2.VideoCapture(video_filepath)\n",
    "            frame_count = 0\n",
    "             \n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, int(frame_start))\n",
    "            \n",
    "            for frame_num in range(int(frame_start), int(frame_end) + 1):\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    print(f\"Frame number {frame_num} is missing.\")\n",
    "                    break\n",
    "                output_filename = f'img_{frame_num:06d}.png'\n",
    "                output_path = os.path.join(updated_output_dir, output_filename)\n",
    "                cv2.imwrite(output_path, frame)\n",
    "                frame_count += 1\n",
    "                \n",
    "                if frame_count > max_frames_per_chunk:\n",
    "                    break\n",
    "\n",
    "            cap.release()\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_id}: {e}\")\n",
    "\n",
    "    def process_annotations(self, dataset_sub_dir, max_frames_per_chunk):\n",
    "        \"\"\"\n",
    "        Processes the annotation file for a dataset split in parallel using multiple processes.\n",
    "        \n",
    "        Args:\n",
    "            dataset_sub_dir (str): The sub-directory (e.g., 'train', 'test') for saving extracted frames.\n",
    "            max_frames_per_chunk (int): The maximum number of frames to extract per chunk.\n",
    "        \"\"\"\n",
    "        annotation_file = self.annotation_files[dataset_sub_dir]\n",
    "        output_dir = os.path.join(self.root_dataset_dir, dataset_sub_dir)\n",
    "        \n",
    "        try:\n",
    "            with open(annotation_file, 'r') as csvfile:\n",
    "                reader = csv.reader(csvfile)\n",
    "                next(reader)  # Skip header row\n",
    "                \n",
    "                with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "                    futures = [executor.submit(self.extract_frames, row, output_dir, max_frames_per_chunk) for row in reader]\n",
    "                    concurrent.futures.wait(futures)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing annotation file {annotation_file}: {e}\")\n",
    "\n",
    "def format_time(seconds):\n",
    "    \"\"\"Converts time in seconds to hours, minutes, and seconds format.\"\"\"\n",
    "    hours = seconds // 3600\n",
    "    minutes = (seconds % 3600) // 60\n",
    "    seconds = seconds % 60\n",
    "    return f\"{int(hours)} hours, {int(minutes)} minutes, {int(seconds)} seconds\"\n",
    "\n",
    "def main():\n",
    "    # path to the video files\n",
    "    data_dir = \"/home/sur06423/hiwi/vit_exp/vision_tranformer_baseline/data/kinect_ir\"\n",
    "    # the output directory where the extracted frames will be stored \n",
    "    root_dataset_dir = \"/net/polaris/storage/deeplearning/sur_data/kinect_ir_daa/split_0\"\n",
    "    annotation_files = {\n",
    "        'train': \"/home/sur06423/hiwi/vit_exp/vision_tranformer_baseline/data/kinect_color_annotation/activities_3s/kinect_ir/midlevel.chunks_90.split_0.train.csv\",\n",
    "        'test': \"/home/sur06423/hiwi/vit_exp/vision_tranformer_baseline/data/kinect_color_annotation/activities_3s/kinect_ir/midlevel.chunks_90.split_0.test.csv\",\n",
    "        'val': \"/home/sur06423/hiwi/vit_exp/vision_tranformer_baseline/data/kinect_color_annotation/activities_3s/kinect_ir/midlevel.chunks_90.split_0.val.csv\"\n",
    "    }\n",
    "\n",
    "    video_frame_extractor = VideoFrameExtractor(data_dir, root_dataset_dir, annotation_files)\n",
    "    start_time = time.time()\n",
    "\n",
    "    for dataset_sub_dir in annotation_files.keys():\n",
    "        # Chunk = 3 seconds video, 1 second = Frame Rate, Frames: seconds x Frame rate\n",
    "        max_frames_per_chunk = 48  # This value can be adjusted as needed according to 15 FPS or 30FPS\n",
    "        print(f\"Processing {dataset_sub_dir} dataset...\")\n",
    "        video_frame_extractor.process_annotations(dataset_sub_dir, max_frames_per_chunk)\n",
    "\n",
    "    end_time = time.time()\n",
    "    time_taken = format_time(end_time - start_time)\n",
    "    print(f\"Time taken for extracting dataset frames: {time_taken}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For split_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing train dataset...\n",
      "Processing test dataset...\n",
      "Processing val dataset...\n",
      "Time taken for extracting dataset frames: 0 hours, 14 minutes, 11 seconds\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import csv\n",
    "import time\n",
    "import concurrent.futures\n",
    "\n",
    "class VideoFrameExtractor:\n",
    "    \"\"\"\n",
    "    A class to extract frames from video files based on annotations and process these annotations in parallel.\n",
    "    \n",
    "    Attributes:\n",
    "        data_dir (str): The directory where the video files are stored.\n",
    "        root_dataset_dir (str): The root directory where the extracted frames will be saved.\n",
    "        annotation_files (dict): A dictionary mapping dataset splits to their corresponding annotation file paths.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir, root_dataset_dir, annotation_files):\n",
    "        \"\"\"\n",
    "        Initializes the VideoFrameExtractor with dataset directories and annotation file paths.\n",
    "        \"\"\"\n",
    "        self.data_dir = data_dir\n",
    "        self.root_dataset_dir = root_dataset_dir\n",
    "        self.annotation_files = annotation_files\n",
    "\n",
    "    def extract_frames(self, row, output_dir, max_frames_per_chunk):\n",
    "        \"\"\"\n",
    "        Extracts frames from a video file based on a row from the annotation file.\n",
    "        \n",
    "        Args:\n",
    "            row (list): A list containing annotation information for a video segment.\n",
    "            output_dir (str): The directory where the extracted frames should be saved.\n",
    "            max_frames_per_chunk (int): The maximum number of frames to extract per chunk.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            participant_id, file_id, annotation_id, frame_start, frame_end, activity, chunk_id = row\n",
    "            video_filepath = os.path.join(self.data_dir, file_id + '.mp4')\n",
    "            new_file_id = file_id.replace(\"/\", \"_\")\n",
    "            updated_output_dir = os.path.join(output_dir, f'{activity}', f'{participant_id}_{new_file_id}_frames_{frame_start}_{frame_end}_ann_{annotation_id}_chunk_{chunk_id}')\n",
    "            \n",
    "            os.makedirs(updated_output_dir, exist_ok=True)\n",
    "            cap = cv2.VideoCapture(video_filepath)\n",
    "            frame_count = 0\n",
    "             \n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, int(frame_start))\n",
    "            \n",
    "            for frame_num in range(int(frame_start), int(frame_end) + 1):\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    print(f\"Frame number {frame_num} is missing.\")\n",
    "                    break\n",
    "                output_filename = f'img_{frame_num:06d}.png'\n",
    "                output_path = os.path.join(updated_output_dir, output_filename)\n",
    "                cv2.imwrite(output_path, frame)\n",
    "                frame_count += 1\n",
    "                \n",
    "                if frame_count > max_frames_per_chunk:\n",
    "                    break\n",
    "\n",
    "            cap.release()\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_id}: {e}\")\n",
    "\n",
    "    def process_annotations(self, dataset_sub_dir, max_frames_per_chunk):\n",
    "        \"\"\"\n",
    "        Processes the annotation file for a dataset split in parallel using multiple processes.\n",
    "        \n",
    "        Args:\n",
    "            dataset_sub_dir (str): The sub-directory (e.g., 'train', 'test') for saving extracted frames.\n",
    "            max_frames_per_chunk (int): The maximum number of frames to extract per chunk.\n",
    "        \"\"\"\n",
    "        annotation_file = self.annotation_files[dataset_sub_dir]\n",
    "        output_dir = os.path.join(self.root_dataset_dir, dataset_sub_dir)\n",
    "        \n",
    "        try:\n",
    "            with open(annotation_file, 'r') as csvfile:\n",
    "                reader = csv.reader(csvfile)\n",
    "                next(reader)  # Skip header row\n",
    "                \n",
    "                with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "                    futures = [executor.submit(self.extract_frames, row, output_dir, max_frames_per_chunk) for row in reader]\n",
    "                    concurrent.futures.wait(futures)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing annotation file {annotation_file}: {e}\")\n",
    "\n",
    "def format_time(seconds):\n",
    "    \"\"\"Converts time in seconds to hours, minutes, and seconds format.\"\"\"\n",
    "    hours = seconds // 3600\n",
    "    minutes = (seconds % 3600) // 60\n",
    "    seconds = seconds % 60\n",
    "    return f\"{int(hours)} hours, {int(minutes)} minutes, {int(seconds)} seconds\"\n",
    "\n",
    "def main():\n",
    "    # path to the video files\n",
    "    data_dir = \"/home/sur06423/hiwi/vit_exp/vision_tranformer_baseline/data/kinect_ir\"\n",
    "    # the output directory where the extracted frames will be stored \n",
    "    root_dataset_dir = \"/net/polaris/storage/deeplearning/sur_data/kinect_ir_daa/split_1\"\n",
    "    annotation_files = {\n",
    "        'train': \"/home/sur06423/hiwi/vit_exp/vision_tranformer_baseline/data/kinect_color_annotation/activities_3s/kinect_ir/midlevel.chunks_90.split_1.train.csv\",\n",
    "        'test': \"/home/sur06423/hiwi/vit_exp/vision_tranformer_baseline/data/kinect_color_annotation/activities_3s/kinect_ir/midlevel.chunks_90.split_1.test.csv\",\n",
    "        'val': \"/home/sur06423/hiwi/vit_exp/vision_tranformer_baseline/data/kinect_color_annotation/activities_3s/kinect_ir/midlevel.chunks_90.split_1.val.csv\"\n",
    "    }\n",
    "\n",
    "    video_frame_extractor = VideoFrameExtractor(data_dir, root_dataset_dir, annotation_files)\n",
    "    start_time = time.time()\n",
    "\n",
    "    for dataset_sub_dir in annotation_files.keys():\n",
    "        # Chunk = 3 seconds video, 1 second = Frame Rate, Frames: seconds x Frame rate\n",
    "        max_frames_per_chunk = 48  # This value can be adjusted as needed according to 15 FPS or 30FPS\n",
    "        print(f\"Processing {dataset_sub_dir} dataset...\")\n",
    "        video_frame_extractor.process_annotations(dataset_sub_dir, max_frames_per_chunk)\n",
    "\n",
    "    end_time = time.time()\n",
    "    time_taken = format_time(end_time - start_time)\n",
    "    print(f\"Time taken for extracting dataset frames: {time_taken}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For split_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing train dataset...\n",
      "Processing test dataset...\n",
      "Processing val dataset...\n",
      "Time taken for extracting dataset frames: 0 hours, 14 minutes, 28 seconds\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import csv\n",
    "import time\n",
    "import concurrent.futures\n",
    "\n",
    "class VideoFrameExtractor:\n",
    "    \"\"\"\n",
    "    A class to extract frames from video files based on annotations and process these annotations in parallel.\n",
    "    \n",
    "    Attributes:\n",
    "        data_dir (str): The directory where the video files are stored.\n",
    "        root_dataset_dir (str): The root directory where the extracted frames will be saved.\n",
    "        annotation_files (dict): A dictionary mapping dataset splits to their corresponding annotation file paths.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir, root_dataset_dir, annotation_files):\n",
    "        \"\"\"\n",
    "        Initializes the VideoFrameExtractor with dataset directories and annotation file paths.\n",
    "        \"\"\"\n",
    "        self.data_dir = data_dir\n",
    "        self.root_dataset_dir = root_dataset_dir\n",
    "        self.annotation_files = annotation_files\n",
    "\n",
    "    def extract_frames(self, row, output_dir, max_frames_per_chunk):\n",
    "        \"\"\"\n",
    "        Extracts frames from a video file based on a row from the annotation file.\n",
    "        \n",
    "        Args:\n",
    "            row (list): A list containing annotation information for a video segment.\n",
    "            output_dir (str): The directory where the extracted frames should be saved.\n",
    "            max_frames_per_chunk (int): The maximum number of frames to extract per chunk.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            participant_id, file_id, annotation_id, frame_start, frame_end, activity, chunk_id = row\n",
    "            video_filepath = os.path.join(self.data_dir, file_id + '.mp4')\n",
    "            new_file_id = file_id.replace(\"/\", \"_\")\n",
    "            updated_output_dir = os.path.join(output_dir, f'{activity}', f'{participant_id}_{new_file_id}_frames_{frame_start}_{frame_end}_ann_{annotation_id}_chunk_{chunk_id}')\n",
    "            \n",
    "            os.makedirs(updated_output_dir, exist_ok=True)\n",
    "            cap = cv2.VideoCapture(video_filepath)\n",
    "            frame_count = 0\n",
    "             \n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, int(frame_start))\n",
    "            \n",
    "            for frame_num in range(int(frame_start), int(frame_end) + 1):\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    print(f\"Frame number {frame_num} is missing.\")\n",
    "                    break\n",
    "                output_filename = f'img_{frame_num:06d}.png'\n",
    "                output_path = os.path.join(updated_output_dir, output_filename)\n",
    "                cv2.imwrite(output_path, frame)\n",
    "                frame_count += 1\n",
    "                \n",
    "                if frame_count > max_frames_per_chunk:\n",
    "                    break\n",
    "\n",
    "            cap.release()\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_id}: {e}\")\n",
    "\n",
    "    def process_annotations(self, dataset_sub_dir, max_frames_per_chunk):\n",
    "        \"\"\"\n",
    "        Processes the annotation file for a dataset split in parallel using multiple processes.\n",
    "        \n",
    "        Args:\n",
    "            dataset_sub_dir (str): The sub-directory (e.g., 'train', 'test') for saving extracted frames.\n",
    "            max_frames_per_chunk (int): The maximum number of frames to extract per chunk.\n",
    "        \"\"\"\n",
    "        annotation_file = self.annotation_files[dataset_sub_dir]\n",
    "        output_dir = os.path.join(self.root_dataset_dir, dataset_sub_dir)\n",
    "        \n",
    "        try:\n",
    "            with open(annotation_file, 'r') as csvfile:\n",
    "                reader = csv.reader(csvfile)\n",
    "                next(reader)  # Skip header row\n",
    "                \n",
    "                with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "                    futures = [executor.submit(self.extract_frames, row, output_dir, max_frames_per_chunk) for row in reader]\n",
    "                    concurrent.futures.wait(futures)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing annotation file {annotation_file}: {e}\")\n",
    "\n",
    "def format_time(seconds):\n",
    "    \"\"\"Converts time in seconds to hours, minutes, and seconds format.\"\"\"\n",
    "    hours = seconds // 3600\n",
    "    minutes = (seconds % 3600) // 60\n",
    "    seconds = seconds % 60\n",
    "    return f\"{int(hours)} hours, {int(minutes)} minutes, {int(seconds)} seconds\"\n",
    "\n",
    "def main():\n",
    "    # path to the video files\n",
    "    data_dir = \"/home/sur06423/hiwi/vit_exp/vision_tranformer_baseline/data/kinect_ir\"\n",
    "    # the output directory where the extracted frames will be stored \n",
    "    root_dataset_dir = \"/net/polaris/storage/deeplearning/sur_data/kinect_ir_daa/split_2\"\n",
    "    annotation_files = {\n",
    "        'train': \"/home/sur06423/hiwi/vit_exp/vision_tranformer_baseline/data/kinect_color_annotation/activities_3s/kinect_ir/midlevel.chunks_90.split_2.train.csv\",\n",
    "        'test': \"/home/sur06423/hiwi/vit_exp/vision_tranformer_baseline/data/kinect_color_annotation/activities_3s/kinect_ir/midlevel.chunks_90.split_2.test.csv\",\n",
    "        'val': \"/home/sur06423/hiwi/vit_exp/vision_tranformer_baseline/data/kinect_color_annotation/activities_3s/kinect_ir/midlevel.chunks_90.split_2.val.csv\"\n",
    "    }\n",
    "\n",
    "    video_frame_extractor = VideoFrameExtractor(data_dir, root_dataset_dir, annotation_files)\n",
    "    start_time = time.time()\n",
    "\n",
    "    for dataset_sub_dir in annotation_files.keys():\n",
    "        # Chunk = 3 seconds video, 1 second = Frame Rate, Frames: seconds x Frame rate\n",
    "        max_frames_per_chunk = 48  # This value can be adjusted as needed according to 15 FPS or 30FPS\n",
    "        print(f\"Processing {dataset_sub_dir} dataset...\")\n",
    "        video_frame_extractor.process_annotations(dataset_sub_dir, max_frames_per_chunk)\n",
    "\n",
    "    end_time = time.time()\n",
    "    time_taken = format_time(end_time - start_time)\n",
    "    print(f\"Time taken for extracting dataset frames: {time_taken}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vi_trans",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
