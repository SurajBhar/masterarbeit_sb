{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nir_annotation_files_split_0 = [\n",
    "    \"/home/sur06423/hiwi/vit_exp/vision_tranformer_baseline/data/kinect_color_annotation/activities_3s/inner_mirror/midlevel.chunks_90.split_0.test.csv\",\n",
    "    \"/home/sur06423/hiwi/vit_exp/vision_tranformer_baseline/data/kinect_color_annotation/activities_3s/inner_mirror/midlevel.chunks_90.split_0.train.csv\",\n",
    "    \"/home/sur06423/hiwi/vit_exp/vision_tranformer_baseline/data/kinect_color_annotation/activities_3s/inner_mirror/midlevel.chunks_90.split_0.val.csv\"\n",
    "]\n",
    "\n",
    "nir_annotation_files_split_1 = [\n",
    "    \"/home/sur06423/hiwi/vit_exp/vision_tranformer_baseline/data/kinect_color_annotation/activities_3s/inner_mirror/midlevel.chunks_90.split_1.test.csv\",\n",
    "    \"/home/sur06423/hiwi/vit_exp/vision_tranformer_baseline/data/kinect_color_annotation/activities_3s/inner_mirror/midlevel.chunks_90.split_1.train.csv\",\n",
    "    \"/home/sur06423/hiwi/vit_exp/vision_tranformer_baseline/data/kinect_color_annotation/activities_3s/inner_mirror/midlevel.chunks_90.split_1.val.csv\"\n",
    "]\n",
    "\n",
    "nir_annotation_files_split_2 = [\n",
    "    \"/home/sur06423/hiwi/vit_exp/vision_tranformer_baseline/data/kinect_color_annotation/activities_3s/inner_mirror/midlevel.chunks_90.split_2.test.csv\",\n",
    "    \"/home/sur06423/hiwi/vit_exp/vision_tranformer_baseline/data/kinect_color_annotation/activities_3s/inner_mirror/midlevel.chunks_90.split_2.train.csv\",\n",
    "    \"/home/sur06423/hiwi/vit_exp/vision_tranformer_baseline/data/kinect_color_annotation/activities_3s/inner_mirror/midlevel.chunks_90.split_2.val.csv\"\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Split_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing train dataset...\n",
      "Processing test dataset...\n",
      "Processing val dataset...\n",
      "Time taken for extracting dataset frames: 1 hours, 28 minutes, 55 seconds\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import csv\n",
    "import time\n",
    "import concurrent.futures\n",
    "\n",
    "class VideoFrameExtractor:\n",
    "    \"\"\"\n",
    "    A class to extract frames from video files based on annotations and process these annotations in parallel.\n",
    "    \n",
    "    Attributes:\n",
    "        data_dir (str): The directory where the video files are stored.\n",
    "        root_dataset_dir (str): The root directory where the extracted frames will be saved.\n",
    "        annotation_files (dict): A dictionary mapping dataset splits to their corresponding annotation file paths.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir, root_dataset_dir, annotation_files):\n",
    "        \"\"\"\n",
    "        Initializes the VideoFrameExtractor with dataset directories and annotation file paths.\n",
    "        \"\"\"\n",
    "        self.data_dir = data_dir\n",
    "        self.root_dataset_dir = root_dataset_dir\n",
    "        self.annotation_files = annotation_files\n",
    "\n",
    "    def extract_frames(self, row, output_dir, max_frames_per_chunk):\n",
    "        \"\"\"\n",
    "        Extracts frames from a video file based on a row from the annotation file.\n",
    "        \n",
    "        Args:\n",
    "            row (list): A list containing annotation information for a video segment.\n",
    "            output_dir (str): The directory where the extracted frames should be saved.\n",
    "            max_frames_per_chunk (int): The maximum number of frames to extract per chunk.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            participant_id, file_id, annotation_id, frame_start, frame_end, activity, chunk_id = row\n",
    "            video_filepath = os.path.join(self.data_dir, file_id + '.mp4')\n",
    "            new_file_id = file_id.replace(\"/\", \"_\")\n",
    "            updated_output_dir = os.path.join(output_dir, f'{activity}', f'{participant_id}_{new_file_id}_frames_{frame_start}_{frame_end}_ann_{annotation_id}_chunk_{chunk_id}')\n",
    "            \n",
    "            os.makedirs(updated_output_dir, exist_ok=True)\n",
    "            cap = cv2.VideoCapture(video_filepath)\n",
    "            frame_count = 0\n",
    "             \n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, int(frame_start))\n",
    "            \n",
    "            for frame_num in range(int(frame_start), int(frame_end) + 1):\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    print(f\"Frame number {frame_num} is missing.\")\n",
    "                    break\n",
    "                output_filename = f'img_{frame_num:06d}.png'\n",
    "                output_path = os.path.join(updated_output_dir, output_filename)\n",
    "                cv2.imwrite(output_path, frame)\n",
    "                frame_count += 1\n",
    "                \n",
    "                if frame_count > max_frames_per_chunk:\n",
    "                    break\n",
    "\n",
    "            cap.release()\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_id}: {e}\")\n",
    "\n",
    "    def process_annotations(self, dataset_sub_dir, max_frames_per_chunk):\n",
    "        \"\"\"\n",
    "        Processes the annotation file for a dataset split in parallel using multiple processes.\n",
    "        \n",
    "        Args:\n",
    "            dataset_sub_dir (str): The sub-directory (e.g., 'train', 'test') for saving extracted frames.\n",
    "            max_frames_per_chunk (int): The maximum number of frames to extract per chunk.\n",
    "        \"\"\"\n",
    "        annotation_file = self.annotation_files[dataset_sub_dir]\n",
    "        output_dir = os.path.join(self.root_dataset_dir, dataset_sub_dir)\n",
    "        \n",
    "        try:\n",
    "            with open(annotation_file, 'r') as csvfile:\n",
    "                reader = csv.reader(csvfile)\n",
    "                next(reader)  # Skip header row\n",
    "                \n",
    "                with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "                    futures = [executor.submit(self.extract_frames, row, output_dir, max_frames_per_chunk) for row in reader]\n",
    "                    concurrent.futures.wait(futures)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing annotation file {annotation_file}: {e}\")\n",
    "\n",
    "def format_time(seconds):\n",
    "    \"\"\"Converts time in seconds to hours, minutes, and seconds format.\"\"\"\n",
    "    hours = seconds // 3600\n",
    "    minutes = (seconds % 3600) // 60\n",
    "    seconds = seconds % 60\n",
    "    return f\"{int(hours)} hours, {int(minutes)} minutes, {int(seconds)} seconds\"\n",
    "\n",
    "def main():\n",
    "    # path to the video files\n",
    "    data_dir = \"/home/sur06423/hiwi/vit_exp/vision_tranformer_baseline/data/inner_mirror\"\n",
    "    # the output directory where the extracted frames will be stored \n",
    "    root_dataset_dir = \"/net/polaris/storage/deeplearning/sur_data/nir_front_top_daa/split_0\"\n",
    "    annotation_files = {\n",
    "        'train': \"/home/sur06423/hiwi/vit_exp/vision_tranformer_baseline/data/kinect_color_annotation/activities_3s/inner_mirror/midlevel.chunks_90.split_0.train.csv\",\n",
    "        'test': \"/home/sur06423/hiwi/vit_exp/vision_tranformer_baseline/data/kinect_color_annotation/activities_3s/inner_mirror/midlevel.chunks_90.split_0.test.csv\",\n",
    "        'val': \"/home/sur06423/hiwi/vit_exp/vision_tranformer_baseline/data/kinect_color_annotation/activities_3s/inner_mirror/midlevel.chunks_90.split_0.val.csv\"\n",
    "    }\n",
    "\n",
    "    video_frame_extractor = VideoFrameExtractor(data_dir, root_dataset_dir, annotation_files)\n",
    "    start_time = time.time()\n",
    "\n",
    "    for dataset_sub_dir in annotation_files.keys():\n",
    "        # Chunk = 3 seconds video, 1 second = Frame Rate, Frames: seconds x Frame rate\n",
    "        max_frames_per_chunk = 95  # This value can be adjusted as needed according to 15 FPS or 30FPS\n",
    "        print(f\"Processing {dataset_sub_dir} dataset...\")\n",
    "        video_frame_extractor.process_annotations(dataset_sub_dir, max_frames_per_chunk)\n",
    "\n",
    "    end_time = time.time()\n",
    "    time_taken = format_time(end_time - start_time)\n",
    "    print(f\"Time taken for extracting dataset frames: {time_taken}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing train dataset...\n",
      "Processing test dataset...\n",
      "Processing val dataset...\n",
      "Time taken for extracting dataset frames: 1 hours, 28 minutes, 33 seconds\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import csv\n",
    "import time\n",
    "import concurrent.futures\n",
    "\n",
    "class VideoFrameExtractor:\n",
    "    \"\"\"\n",
    "    A class to extract frames from video files based on annotations and process these annotations in parallel.\n",
    "    \n",
    "    Attributes:\n",
    "        data_dir (str): The directory where the video files are stored.\n",
    "        root_dataset_dir (str): The root directory where the extracted frames will be saved.\n",
    "        annotation_files (dict): A dictionary mapping dataset splits to their corresponding annotation file paths.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir, root_dataset_dir, annotation_files):\n",
    "        \"\"\"\n",
    "        Initializes the VideoFrameExtractor with dataset directories and annotation file paths.\n",
    "        \"\"\"\n",
    "        self.data_dir = data_dir\n",
    "        self.root_dataset_dir = root_dataset_dir\n",
    "        self.annotation_files = annotation_files\n",
    "\n",
    "    def extract_frames(self, row, output_dir, max_frames_per_chunk):\n",
    "        \"\"\"\n",
    "        Extracts frames from a video file based on a row from the annotation file.\n",
    "        \n",
    "        Args:\n",
    "            row (list): A list containing annotation information for a video segment.\n",
    "            output_dir (str): The directory where the extracted frames should be saved.\n",
    "            max_frames_per_chunk (int): The maximum number of frames to extract per chunk.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            participant_id, file_id, annotation_id, frame_start, frame_end, activity, chunk_id = row\n",
    "            video_filepath = os.path.join(self.data_dir, file_id + '.mp4')\n",
    "            new_file_id = file_id.replace(\"/\", \"_\")\n",
    "            updated_output_dir = os.path.join(output_dir, f'{activity}', f'{participant_id}_{new_file_id}_frames_{frame_start}_{frame_end}_ann_{annotation_id}_chunk_{chunk_id}')\n",
    "            \n",
    "            os.makedirs(updated_output_dir, exist_ok=True)\n",
    "            cap = cv2.VideoCapture(video_filepath)\n",
    "            frame_count = 0\n",
    "             \n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, int(frame_start))\n",
    "            \n",
    "            for frame_num in range(int(frame_start), int(frame_end) + 1):\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    print(f\"Frame number {frame_num} is missing.\")\n",
    "                    break\n",
    "                output_filename = f'img_{frame_num:06d}.png'\n",
    "                output_path = os.path.join(updated_output_dir, output_filename)\n",
    "                cv2.imwrite(output_path, frame)\n",
    "                frame_count += 1\n",
    "                \n",
    "                if frame_count > max_frames_per_chunk:\n",
    "                    break\n",
    "\n",
    "            cap.release()\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_id}: {e}\")\n",
    "\n",
    "    def process_annotations(self, dataset_sub_dir, max_frames_per_chunk):\n",
    "        \"\"\"\n",
    "        Processes the annotation file for a dataset split in parallel using multiple processes.\n",
    "        \n",
    "        Args:\n",
    "            dataset_sub_dir (str): The sub-directory (e.g., 'train', 'test') for saving extracted frames.\n",
    "            max_frames_per_chunk (int): The maximum number of frames to extract per chunk.\n",
    "        \"\"\"\n",
    "        annotation_file = self.annotation_files[dataset_sub_dir]\n",
    "        output_dir = os.path.join(self.root_dataset_dir, dataset_sub_dir)\n",
    "        \n",
    "        try:\n",
    "            with open(annotation_file, 'r') as csvfile:\n",
    "                reader = csv.reader(csvfile)\n",
    "                next(reader)  # Skip header row\n",
    "                \n",
    "                with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "                    futures = [executor.submit(self.extract_frames, row, output_dir, max_frames_per_chunk) for row in reader]\n",
    "                    concurrent.futures.wait(futures)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing annotation file {annotation_file}: {e}\")\n",
    "\n",
    "def format_time(seconds):\n",
    "    \"\"\"Converts time in seconds to hours, minutes, and seconds format.\"\"\"\n",
    "    hours = seconds // 3600\n",
    "    minutes = (seconds % 3600) // 60\n",
    "    seconds = seconds % 60\n",
    "    return f\"{int(hours)} hours, {int(minutes)} minutes, {int(seconds)} seconds\"\n",
    "\n",
    "def main():\n",
    "    # path to the video files\n",
    "    data_dir = \"/home/sur06423/hiwi/vit_exp/vision_tranformer_baseline/data/inner_mirror\"\n",
    "    # the output directory where the extracted frames will be stored \n",
    "    root_dataset_dir = \"/net/polaris/storage/deeplearning/sur_data/nir_front_top_daa/split_1\"\n",
    "    annotation_files = {\n",
    "        'train': \"/home/sur06423/hiwi/vit_exp/vision_tranformer_baseline/data/kinect_color_annotation/activities_3s/inner_mirror/midlevel.chunks_90.split_1.train.csv\",\n",
    "        'test': \"/home/sur06423/hiwi/vit_exp/vision_tranformer_baseline/data/kinect_color_annotation/activities_3s/inner_mirror/midlevel.chunks_90.split_1.test.csv\",\n",
    "        'val': \"/home/sur06423/hiwi/vit_exp/vision_tranformer_baseline/data/kinect_color_annotation/activities_3s/inner_mirror/midlevel.chunks_90.split_1.val.csv\"\n",
    "    }\n",
    "\n",
    "    video_frame_extractor = VideoFrameExtractor(data_dir, root_dataset_dir, annotation_files)\n",
    "    start_time = time.time()\n",
    "\n",
    "    for dataset_sub_dir in annotation_files.keys():\n",
    "        # Chunk = 3 seconds video, 1 second = Frame Rate, Frames: seconds x Frame rate\n",
    "        max_frames_per_chunk = 95  # This value can be adjusted as needed according to 15 FPS or 30FPS\n",
    "        print(f\"Processing {dataset_sub_dir} dataset...\")\n",
    "        video_frame_extractor.process_annotations(dataset_sub_dir, max_frames_per_chunk)\n",
    "\n",
    "    end_time = time.time()\n",
    "    time_taken = format_time(end_time - start_time)\n",
    "    print(f\"Time taken for extracting dataset frames: {time_taken}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing train dataset...\n",
      "Processing test dataset...\n",
      "Processing val dataset...\n",
      "Time taken for extracting dataset frames: 1 hours, 27 minutes, 57 seconds\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import csv\n",
    "import time\n",
    "import concurrent.futures\n",
    "\n",
    "class VideoFrameExtractor:\n",
    "    \"\"\"\n",
    "    A class to extract frames from video files based on annotations and process these annotations in parallel.\n",
    "    \n",
    "    Attributes:\n",
    "        data_dir (str): The directory where the video files are stored.\n",
    "        root_dataset_dir (str): The root directory where the extracted frames will be saved.\n",
    "        annotation_files (dict): A dictionary mapping dataset splits to their corresponding annotation file paths.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir, root_dataset_dir, annotation_files):\n",
    "        \"\"\"\n",
    "        Initializes the VideoFrameExtractor with dataset directories and annotation file paths.\n",
    "        \"\"\"\n",
    "        self.data_dir = data_dir\n",
    "        self.root_dataset_dir = root_dataset_dir\n",
    "        self.annotation_files = annotation_files\n",
    "\n",
    "    def extract_frames(self, row, output_dir, max_frames_per_chunk):\n",
    "        \"\"\"\n",
    "        Extracts frames from a video file based on a row from the annotation file.\n",
    "        \n",
    "        Args:\n",
    "            row (list): A list containing annotation information for a video segment.\n",
    "            output_dir (str): The directory where the extracted frames should be saved.\n",
    "            max_frames_per_chunk (int): The maximum number of frames to extract per chunk.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            participant_id, file_id, annotation_id, frame_start, frame_end, activity, chunk_id = row\n",
    "            video_filepath = os.path.join(self.data_dir, file_id + '.mp4')\n",
    "            new_file_id = file_id.replace(\"/\", \"_\")\n",
    "            updated_output_dir = os.path.join(output_dir, f'{activity}', f'{participant_id}_{new_file_id}_frames_{frame_start}_{frame_end}_ann_{annotation_id}_chunk_{chunk_id}')\n",
    "            \n",
    "            os.makedirs(updated_output_dir, exist_ok=True)\n",
    "            cap = cv2.VideoCapture(video_filepath)\n",
    "            frame_count = 0\n",
    "             \n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, int(frame_start))\n",
    "            \n",
    "            for frame_num in range(int(frame_start), int(frame_end) + 1):\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    print(f\"Frame number {frame_num} is missing.\")\n",
    "                    break\n",
    "                output_filename = f'img_{frame_num:06d}.png'\n",
    "                output_path = os.path.join(updated_output_dir, output_filename)\n",
    "                cv2.imwrite(output_path, frame)\n",
    "                frame_count += 1\n",
    "                \n",
    "                if frame_count > max_frames_per_chunk:\n",
    "                    break\n",
    "\n",
    "            cap.release()\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_id}: {e}\")\n",
    "\n",
    "    def process_annotations(self, dataset_sub_dir, max_frames_per_chunk):\n",
    "        \"\"\"\n",
    "        Processes the annotation file for a dataset split in parallel using multiple processes.\n",
    "        \n",
    "        Args:\n",
    "            dataset_sub_dir (str): The sub-directory (e.g., 'train', 'test') for saving extracted frames.\n",
    "            max_frames_per_chunk (int): The maximum number of frames to extract per chunk.\n",
    "        \"\"\"\n",
    "        annotation_file = self.annotation_files[dataset_sub_dir]\n",
    "        output_dir = os.path.join(self.root_dataset_dir, dataset_sub_dir)\n",
    "        \n",
    "        try:\n",
    "            with open(annotation_file, 'r') as csvfile:\n",
    "                reader = csv.reader(csvfile)\n",
    "                next(reader)  # Skip header row\n",
    "                \n",
    "                with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "                    futures = [executor.submit(self.extract_frames, row, output_dir, max_frames_per_chunk) for row in reader]\n",
    "                    concurrent.futures.wait(futures)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing annotation file {annotation_file}: {e}\")\n",
    "\n",
    "def format_time(seconds):\n",
    "    \"\"\"Converts time in seconds to hours, minutes, and seconds format.\"\"\"\n",
    "    hours = seconds // 3600\n",
    "    minutes = (seconds % 3600) // 60\n",
    "    seconds = seconds % 60\n",
    "    return f\"{int(hours)} hours, {int(minutes)} minutes, {int(seconds)} seconds\"\n",
    "\n",
    "def main():\n",
    "    # path to the video files\n",
    "    data_dir = \"/home/sur06423/hiwi/vit_exp/vision_tranformer_baseline/data/inner_mirror\"\n",
    "    # the output directory where the extracted frames will be stored \n",
    "    root_dataset_dir = \"/net/polaris/storage/deeplearning/sur_data/nir_front_top_daa/split_2\"\n",
    "    annotation_files = {\n",
    "        'train': \"/home/sur06423/hiwi/vit_exp/vision_tranformer_baseline/data/kinect_color_annotation/activities_3s/inner_mirror/midlevel.chunks_90.split_2.train.csv\",\n",
    "        'test': \"/home/sur06423/hiwi/vit_exp/vision_tranformer_baseline/data/kinect_color_annotation/activities_3s/inner_mirror/midlevel.chunks_90.split_2.test.csv\",\n",
    "        'val': \"/home/sur06423/hiwi/vit_exp/vision_tranformer_baseline/data/kinect_color_annotation/activities_3s/inner_mirror/midlevel.chunks_90.split_2.val.csv\"\n",
    "    }\n",
    "\n",
    "    video_frame_extractor = VideoFrameExtractor(data_dir, root_dataset_dir, annotation_files)\n",
    "    start_time = time.time()\n",
    "\n",
    "    for dataset_sub_dir in annotation_files.keys():\n",
    "        # Chunk = 3 seconds video, 1 second = Frame Rate, Frames: seconds x Frame rate\n",
    "        max_frames_per_chunk = 95  # This value can be adjusted as needed according to 15 FPS or 30FPS\n",
    "        print(f\"Processing {dataset_sub_dir} dataset...\")\n",
    "        video_frame_extractor.process_annotations(dataset_sub_dir, max_frames_per_chunk)\n",
    "\n",
    "    end_time = time.time()\n",
    "    time_taken = format_time(end_time - start_time)\n",
    "    print(f\"Time taken for extracting dataset frames: {time_taken}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vi_trans",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
