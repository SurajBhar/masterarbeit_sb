{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_dir = \"/net/polaris/storage/deeplearning/sur_data/binary_rgb_daa/split_0/train\"\n",
    "new_val_dir = \"/net/polaris/storage/deeplearning/sur_data/binary_rgb_daa/split_0/val\"\n",
    "new_test_dir = \"/net/polaris/storage/deeplearning/sur_data/binary_rgb_daa/split_0/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "\n",
    "pretrained_vit_weights = torchvision.models.ViT_B_16_Weights.DEFAULT\n",
    "pretrained_vit = torchvision.models.vit_b_16(weights=pretrained_vit_weights)\n",
    "\n",
    "# Freeze the base parameters\n",
    "for parameter in pretrained_vit.parameters():\n",
    "    parameter.requires_grad = False\n",
    "\n",
    "# Change the classifier head to match with binary classification:\n",
    "# {distracted_driver, non_distracted_driver}\n",
    "pretrained_vit.heads = nn.Linear(in_features=768, out_features=2)\n",
    "pretrained_vit_transforms = pretrained_vit_weights.transforms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "train_dataset = ImageFolder(root=new_train_dir, transform=pretrained_vit_transforms)\n",
    "val_dataset = ImageFolder(root=new_val_dir, transform=pretrained_vit_transforms)\n",
    "test_dataset = ImageFolder(root=new_test_dir, transform=pretrained_vit_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the Train dataset split_0 RGB is: 259865\n",
      "The length of the Validation dataset split_0 RGB is: 56024\n",
      "The length of the Test dataset split_0 RGB is: 87315\n"
     ]
    }
   ],
   "source": [
    "print(f\"The length of the Train dataset split_0 RGB is: {len(train_dataset)}\")\n",
    "print(f\"The length of the Validation dataset split_0 RGB is: {len(val_dataset)}\")\n",
    "print(f\"The length of the Test dataset split_0 RGB is: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader at 0x7f1335028310>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x7f132f413ac0>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x7f132f4138e0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Turn train and test Datasets into DataLoaders\n",
    "from torch.utils.data import DataLoader\n",
    "train_dataloader = DataLoader(dataset=train_dataset, \n",
    "                              batch_size=1, # how many samples per batch?\n",
    "                              num_workers=1, # how many subprocesses to use for data loading? (higher = more)\n",
    "                              shuffle=True) # shuffle the data?\n",
    "\n",
    "val_dataloader = DataLoader(dataset=val_dataset, \n",
    "                             batch_size=1, \n",
    "                             num_workers=1, \n",
    "                             shuffle=False) # don't usually need to shuffle testing data\n",
    "\n",
    "test_dataloader = DataLoader(dataset=test_dataset, \n",
    "                             batch_size=1, \n",
    "                             num_workers=1, \n",
    "                             shuffle=False) # don't usually need to shuffle testing data\n",
    "\n",
    "train_dataloader, val_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: torch.Size([1, 3, 224, 224]) -> [batch_size, color_channels, height, width]\n",
      "Label shape: torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "img, label = next(iter(train_dataloader))\n",
    "\n",
    "# Batch size will now be 1, try changing the batch_size parameter above and see what happens\n",
    "print(f\"Image shape: {img.shape} -> [batch_size, color_channels, height, width]\")\n",
    "print(f\"Label shape: {label.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install torchinfo if it's not available, import it if it is\n",
    "try: \n",
    "    import torchinfo\n",
    "except:\n",
    "    !pip install torchinfo\n",
    "    import torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sur06423/miniconda3/envs/vi_trans/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at /opt/conda/conda-bld/pytorch_1704987280714/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "VisionTransformer                             [1, 2]                    768\n",
       "├─Conv2d: 1-1                                 [1, 768, 14, 14]          (590,592)\n",
       "├─Encoder: 1-2                                [1, 197, 768]             151,296\n",
       "│    └─Dropout: 2-1                           [1, 197, 768]             --\n",
       "│    └─Sequential: 2-2                        [1, 197, 768]             --\n",
       "│    │    └─EncoderBlock: 3-1                 [1, 197, 768]             (7,087,872)\n",
       "│    │    └─EncoderBlock: 3-2                 [1, 197, 768]             (7,087,872)\n",
       "│    │    └─EncoderBlock: 3-3                 [1, 197, 768]             (7,087,872)\n",
       "│    │    └─EncoderBlock: 3-4                 [1, 197, 768]             (7,087,872)\n",
       "│    │    └─EncoderBlock: 3-5                 [1, 197, 768]             (7,087,872)\n",
       "│    │    └─EncoderBlock: 3-6                 [1, 197, 768]             (7,087,872)\n",
       "│    │    └─EncoderBlock: 3-7                 [1, 197, 768]             (7,087,872)\n",
       "│    │    └─EncoderBlock: 3-8                 [1, 197, 768]             (7,087,872)\n",
       "│    │    └─EncoderBlock: 3-9                 [1, 197, 768]             (7,087,872)\n",
       "│    │    └─EncoderBlock: 3-10                [1, 197, 768]             (7,087,872)\n",
       "│    │    └─EncoderBlock: 3-11                [1, 197, 768]             (7,087,872)\n",
       "│    │    └─EncoderBlock: 3-12                [1, 197, 768]             (7,087,872)\n",
       "│    └─LayerNorm: 2-3                         [1, 197, 768]             (1,536)\n",
       "├─Linear: 1-3                                 [1, 2]                    1,538\n",
       "===============================================================================================\n",
       "Total params: 85,800,194\n",
       "Trainable params: 1,538\n",
       "Non-trainable params: 85,798,656\n",
       "Total mult-adds (M): 172.47\n",
       "===============================================================================================\n",
       "Input size (MB): 0.60\n",
       "Forward/backward pass size (MB): 104.09\n",
       "Params size (MB): 229.20\n",
       "Estimated Total Size (MB): 333.89\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "summary(pretrained_vit, input_size=[1, 3, 224, 224])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "VisionTransformer                             [512, 2]                  768\n",
       "├─Conv2d: 1-1                                 [512, 768, 14, 14]        (590,592)\n",
       "├─Encoder: 1-2                                [512, 197, 768]           151,296\n",
       "│    └─Dropout: 2-1                           [512, 197, 768]           --\n",
       "│    └─Sequential: 2-2                        [512, 197, 768]           --\n",
       "│    │    └─EncoderBlock: 3-1                 [512, 197, 768]           (7,087,872)\n",
       "│    │    └─EncoderBlock: 3-2                 [512, 197, 768]           (7,087,872)\n",
       "│    │    └─EncoderBlock: 3-3                 [512, 197, 768]           (7,087,872)\n",
       "│    │    └─EncoderBlock: 3-4                 [512, 197, 768]           (7,087,872)\n",
       "│    │    └─EncoderBlock: 3-5                 [512, 197, 768]           (7,087,872)\n",
       "│    │    └─EncoderBlock: 3-6                 [512, 197, 768]           (7,087,872)\n",
       "│    │    └─EncoderBlock: 3-7                 [512, 197, 768]           (7,087,872)\n",
       "│    │    └─EncoderBlock: 3-8                 [512, 197, 768]           (7,087,872)\n",
       "│    │    └─EncoderBlock: 3-9                 [512, 197, 768]           (7,087,872)\n",
       "│    │    └─EncoderBlock: 3-10                [512, 197, 768]           (7,087,872)\n",
       "│    │    └─EncoderBlock: 3-11                [512, 197, 768]           (7,087,872)\n",
       "│    │    └─EncoderBlock: 3-12                [512, 197, 768]           (7,087,872)\n",
       "│    └─LayerNorm: 2-3                         [512, 197, 768]           (1,536)\n",
       "├─Linear: 1-3                                 [512, 2]                  1,538\n",
       "===============================================================================================\n",
       "Total params: 85,800,194\n",
       "Trainable params: 1,538\n",
       "Non-trainable params: 85,798,656\n",
       "Total mult-adds (G): 88.30\n",
       "===============================================================================================\n",
       "Input size (MB): 308.28\n",
       "Forward/backward pass size (MB): 53291.79\n",
       "Params size (MB): 229.20\n",
       "Estimated Total Size (MB): 53829.27\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pass through a batchsize of 512\n",
    "from torchinfo import summary\n",
    "summary(pretrained_vit, input_size=[512, 3, 224, 224])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vi_trans",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
