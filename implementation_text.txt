This file contains all the text required for writing about implementation details.

Feature Extraction vs. Fine-Tuning:
Feature Extraction: You take a pre-trained model (pre-trained on a large benchmark dataset like ImageNet), 
freeze the weights of most or all of its layers, and only retrain the final layer(s) to make predictions on 
your specific task. Since the pre-trained layers are not updated during training, they act as a feature extractor, 
and only the features they extract are used to train the new classifier layer. This approach is efficient when 
the new dataset is small but similar to the original dataset the model was trained on.

Fine-Tuning: Unlike feature extraction, fine-tuning involves unfreezing the weights of the pre-trained layers 
(fully or partially) and continuing the training process on the new dataset. This allows the pre-trained model 
to adjust its weights (fine-tune them) to the new data. Fine-tuning typically requires a larger dataset to
avoid overfitting, as more parameters are being adjusted.

What we're doing is a feature extraction approach. We're effectively leveraging the pre-trained layers of the 
Vision Transformer as a fixed feature extractor, and only the weights of the new classification layer we've added 
will be learned from your new DAA dataset. This method allows us to benefit from the powerful representations learned 
by the Vision Transformer on ImageNet 1K dataset, even when our dataset might be too small or too different to fine-tune 
the entire model effectively.
_____________________________________________________

Recall is a metric that measures how often a machine learning model correctly identifies positive instances (true positives) 
from all the actual positive samples in the dataset. You can calculate recall by dividing the number of true positives by 
the number of positive instances. The latter includes true positives (successfully identified cases) and false negative results 
(missed cases).
Recall can also be called sensitivity or true positive rate. 
The term "sensitivity" is more commonly used in medical and biological research rather than machine learning. 

Recall = (TP/ (TP+FN))
__________________________________________________________________________________________________________________________________

To incorporate gradient clipping by global norm in your training loop, specifically with a clipping value of 1, 
you should apply the gradient clipping right after loss.backward() and before self.optimizer.step(). Gradient clipping is a technique 
used to prevent exploding gradients in neural networks, which can destabilize the training process. It does so by scaling the gradients 
of the parameters if the norm exceeds a certain threshold.
_____________________________________________________

Supervised- Feature Extraction Baseline (Vision Transformer Model):
Vision Transformer : ViT_B_16
Pretrained on : Imagenet 1K Dataset

The model builder above accepts the following values as the weights parameter. 
ViT_B_16_Weights.DEFAULT is equivalent to ViT_B_16_Weights.IMAGENET1K_V1. 
You can also use strings, e.g. weights='DEFAULT' or weights='IMAGENET1K_V1'.


_______________________ Modified for DAA _________________

import torch.nn as nn

pretrained_vit_weights = torchvision.models.ViT_B_16_Weights.DEFAULT
pretrained_vit = torchvision.models.vit_b_16(weights = pretrained_vit_weights).to(device)
# Freeze the base parameters
for parameter in pretrained_vit.parameters():
    parameter.requires_grad = False

# Change the classifier head
pretrained_vit.heads = nn.Linear(in_features=768, out_features=34)

# Print a summary of our custom ViT model using torchinfo (uncomment for actual output)
summary(model=pretrained_vit, 
         input_size=(1, 3, 224, 224), # (batch_size, color_channels, height, width)
         #col_names=["input_size"], # uncomment for smaller output
         col_names=["input_size", "output_size", "num_params", "trainable"],
         col_width=20,
         row_settings=["var_names"]
)
============================================================================================================================================
Layer (type (var_name))                                      Input Shape          Output Shape         Param #              Trainable
============================================================================================================================================
VisionTransformer (VisionTransformer)                        [1, 3, 224, 224]     [1, 34]              768                  Partial
├─Conv2d (conv_proj)                                         [1, 3, 224, 224]     [1, 768, 14, 14]     (590,592)            False
├─Encoder (encoder)                                          [1, 197, 768]        [1, 197, 768]        151,296              False
│    └─Dropout (dropout)                                     [1, 197, 768]        [1, 197, 768]        --                   --
│    └─Sequential (layers)                                   [1, 197, 768]        [1, 197, 768]        --                   False
│    │    └─EncoderBlock (encoder_layer_0)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False
│    │    └─EncoderBlock (encoder_layer_1)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False
│    │    └─EncoderBlock (encoder_layer_2)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False
│    │    └─EncoderBlock (encoder_layer_3)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False
│    │    └─EncoderBlock (encoder_layer_4)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False
│    │    └─EncoderBlock (encoder_layer_5)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False
│    │    └─EncoderBlock (encoder_layer_6)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False
│    │    └─EncoderBlock (encoder_layer_7)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False
│    │    └─EncoderBlock (encoder_layer_8)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False
│    │    └─EncoderBlock (encoder_layer_9)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False
│    │    └─EncoderBlock (encoder_layer_10)                  [1, 197, 768]        [1, 197, 768]        (7,087,872)          False
│    │    └─EncoderBlock (encoder_layer_11)                  [1, 197, 768]        [1, 197, 768]        (7,087,872)          False
│    └─LayerNorm (ln)                                        [1, 197, 768]        [1, 197, 768]        (1,536)              False
├─Linear (heads)                                             [1, 768]             [1, 34]              26,146               True
============================================================================================================================================
Total params: 85,824,802
Trainable params: 26,146
...
Input size (MB): 0.60
Forward/backward pass size (MB): 104.09
Params size (MB): 229.30
Estimated Total Size (MB): 333.99

__________________________________________________________________________________________________________________________________

Learning rate sweep: refers to the process of systematically trying out different learning rates 
    from a specified set to identify which learning rate yields the best performance on a given task, 
    such as image classification on ImageNet. This approach is a form of hyperparameter tuning specifically 
    focused on finding the optimal learning rate for training the model.
    The process involves training separate models (or the same model multiple times) with each of the specified learning rates 
    (0.003, 0.01, 0.03, 0.06) for a predefined number of steps (20,000 in this case) and 
    then comparing their performance on a validation or development set. 
    The learning rate that leads to the best performance is then chosen for the final training phase on the entire training set, 
    and the model is subsequently evaluated on the test set.

Step: A step is a single iteration where the model is updated once with a batch of data. 
Epoch: An epoch is completed when the model has been trained on every sample in the training dataset once. 

Therefore, when the ViT paper mentions "20,000" steps for fine tuning on Imagenet dataset,
it means the model's weights are updated 20,000 times, with each update using a different batch of data from the training set. 
The number of epochs this translates into depends on the size of your dataset and the batch size you're using.

DAA Dataset: 
    With a training dataset size of 2,59,865 for DAA Dataset and a batch size of 1024, 
    it takes approximately 254 steps to complete one epoch. 

    To reach 20,000 steps, we will be training the model for approximately 79 epochs.

From ViT Paper:
    When transferring ViT models to another dataset, we remove the whole head (two linear layers) and
    replace it by a single, zero-initialized linear layer outputting the number of classes required by the
    target dataset. We found this to be a little more robust than simply re-initializing the very last layer.

    We fine-tune all ViT models using SGD with a momentum of 0.9.

Hyperparameters for fine-tuning (ViT paper): 
    All models are fine-tuned with cosine learning rate decay,
    a batch size of 512, 
    no weight decay, 
    and grad clipping at global norm 1.
    If not mentioned otherwise, fine-tuning resolution is 384.

Dataset: ImageNet | Steps: 20,000 | BaseLR {0.003, 0.01, 0.03, 0.06}

Hyperparameters for fine-tuning (DAA Dataset):
    SGD with a momentum of 0.9, 
    cosine learning rate decay,
    a batch size of 1024, 
    no weight decay, 
    and grad clipping at global norm 1.
    fine-tuning resolution is 224.
    Epochs>79: 90 | Steps: 22,860
    No Grid Search for Best LR.
    Base LR: 0.003
