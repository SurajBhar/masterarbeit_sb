{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54533\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "def find_free_port():\n",
    "    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
    "        s.bind((\"\",0))\n",
    "        s.listen(1)\n",
    "        port = s.getsockname()[1]\n",
    "        return port\n",
    "\n",
    "print(find_free_port())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.2.0\n",
      "torchvision version: 0.17.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "print(f\"torch version: {torch.__version__}\")\n",
    "print(f\"torchvision version: {torchvision.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from torchinfo import summary\n",
    "except:\n",
    "    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n",
    "    !pip install -q torchinfo\n",
    "    from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"/net/polaris/storage/deeplearning/sur_data/binary_rgb_daa/split_0/train\"\n",
    "val_dir = \"/net/polaris/storage/deeplearning/sur_data/binary_rgb_daa/split_0/val\"\n",
    "test_dir = \"/net/polaris/storage/deeplearning/sur_data/binary_rgb_daa/split_0/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/sur06423/.cache/torch/hub/facebookresearch_dinov2_main\n",
      "/home/sur06423/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:43: UserWarning: xFormers is available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is available (SwiGLU)\")\n",
      "/home/sur06423/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:27: UserWarning: xFormers is available (Attention)\n",
      "  warnings.warn(\"xFormers is available (Attention)\")\n",
      "/home/sur06423/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:33: UserWarning: xFormers is available (Block)\n",
      "  warnings.warn(\"xFormers is available (Block)\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "dinov2_vitb14 = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitb14')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==================================================================================================================================\n",
       "Layer (type (var_name))                            Input Shape          Output Shape         Param #              Trainable\n",
       "==================================================================================================================================\n",
       "DinoVisionTransformer (DinoVisionTransformer)      [1024, 3, 224, 224]  [1024, 768]          1,053,696            True\n",
       "├─PatchEmbed (patch_embed)                         [1024, 3, 224, 224]  [1024, 256, 768]     --                   True\n",
       "│    └─Conv2d (proj)                               [1024, 3, 224, 224]  [1024, 768, 16, 16]  452,352              True\n",
       "│    └─Identity (norm)                             [1024, 256, 768]     [1024, 256, 768]     --                   --\n",
       "├─ModuleList (blocks)                              --                   --                   --                   True\n",
       "│    └─NestedTensorBlock (0)                       [1024, 257, 768]     [1024, 257, 768]     --                   True\n",
       "│    │    └─LayerNorm (norm1)                      [1024, 257, 768]     [1024, 257, 768]     1,536                True\n",
       "│    │    └─MemEffAttention (attn)                 [1024, 257, 768]     [1024, 257, 768]     2,362,368            True\n",
       "│    │    └─LayerScale (ls1)                       [1024, 257, 768]     [1024, 257, 768]     768                  True\n",
       "│    │    └─LayerNorm (norm2)                      [1024, 257, 768]     [1024, 257, 768]     1,536                True\n",
       "│    │    └─Mlp (mlp)                              [1024, 257, 768]     [1024, 257, 768]     4,722,432            True\n",
       "│    │    └─LayerScale (ls2)                       [1024, 257, 768]     [1024, 257, 768]     768                  True\n",
       "│    └─NestedTensorBlock (1)                       [1024, 257, 768]     [1024, 257, 768]     --                   True\n",
       "│    │    └─LayerNorm (norm1)                      [1024, 257, 768]     [1024, 257, 768]     1,536                True\n",
       "│    │    └─MemEffAttention (attn)                 [1024, 257, 768]     [1024, 257, 768]     2,362,368            True\n",
       "│    │    └─LayerScale (ls1)                       [1024, 257, 768]     [1024, 257, 768]     768                  True\n",
       "│    │    └─LayerNorm (norm2)                      [1024, 257, 768]     [1024, 257, 768]     1,536                True\n",
       "│    │    └─Mlp (mlp)                              [1024, 257, 768]     [1024, 257, 768]     4,722,432            True\n",
       "│    │    └─LayerScale (ls2)                       [1024, 257, 768]     [1024, 257, 768]     768                  True\n",
       "│    └─NestedTensorBlock (2)                       [1024, 257, 768]     [1024, 257, 768]     --                   True\n",
       "│    │    └─LayerNorm (norm1)                      [1024, 257, 768]     [1024, 257, 768]     1,536                True\n",
       "│    │    └─MemEffAttention (attn)                 [1024, 257, 768]     [1024, 257, 768]     2,362,368            True\n",
       "│    │    └─LayerScale (ls1)                       [1024, 257, 768]     [1024, 257, 768]     768                  True\n",
       "│    │    └─LayerNorm (norm2)                      [1024, 257, 768]     [1024, 257, 768]     1,536                True\n",
       "│    │    └─Mlp (mlp)                              [1024, 257, 768]     [1024, 257, 768]     4,722,432            True\n",
       "│    │    └─LayerScale (ls2)                       [1024, 257, 768]     [1024, 257, 768]     768                  True\n",
       "│    └─NestedTensorBlock (3)                       [1024, 257, 768]     [1024, 257, 768]     --                   True\n",
       "│    │    └─LayerNorm (norm1)                      [1024, 257, 768]     [1024, 257, 768]     1,536                True\n",
       "│    │    └─MemEffAttention (attn)                 [1024, 257, 768]     [1024, 257, 768]     2,362,368            True\n",
       "│    │    └─LayerScale (ls1)                       [1024, 257, 768]     [1024, 257, 768]     768                  True\n",
       "│    │    └─LayerNorm (norm2)                      [1024, 257, 768]     [1024, 257, 768]     1,536                True\n",
       "│    │    └─Mlp (mlp)                              [1024, 257, 768]     [1024, 257, 768]     4,722,432            True\n",
       "│    │    └─LayerScale (ls2)                       [1024, 257, 768]     [1024, 257, 768]     768                  True\n",
       "│    └─NestedTensorBlock (4)                       [1024, 257, 768]     [1024, 257, 768]     --                   True\n",
       "│    │    └─LayerNorm (norm1)                      [1024, 257, 768]     [1024, 257, 768]     1,536                True\n",
       "│    │    └─MemEffAttention (attn)                 [1024, 257, 768]     [1024, 257, 768]     2,362,368            True\n",
       "│    │    └─LayerScale (ls1)                       [1024, 257, 768]     [1024, 257, 768]     768                  True\n",
       "│    │    └─LayerNorm (norm2)                      [1024, 257, 768]     [1024, 257, 768]     1,536                True\n",
       "│    │    └─Mlp (mlp)                              [1024, 257, 768]     [1024, 257, 768]     4,722,432            True\n",
       "│    │    └─LayerScale (ls2)                       [1024, 257, 768]     [1024, 257, 768]     768                  True\n",
       "│    └─NestedTensorBlock (5)                       [1024, 257, 768]     [1024, 257, 768]     --                   True\n",
       "│    │    └─LayerNorm (norm1)                      [1024, 257, 768]     [1024, 257, 768]     1,536                True\n",
       "│    │    └─MemEffAttention (attn)                 [1024, 257, 768]     [1024, 257, 768]     2,362,368            True\n",
       "│    │    └─LayerScale (ls1)                       [1024, 257, 768]     [1024, 257, 768]     768                  True\n",
       "│    │    └─LayerNorm (norm2)                      [1024, 257, 768]     [1024, 257, 768]     1,536                True\n",
       "│    │    └─Mlp (mlp)                              [1024, 257, 768]     [1024, 257, 768]     4,722,432            True\n",
       "│    │    └─LayerScale (ls2)                       [1024, 257, 768]     [1024, 257, 768]     768                  True\n",
       "│    └─NestedTensorBlock (6)                       [1024, 257, 768]     [1024, 257, 768]     --                   True\n",
       "│    │    └─LayerNorm (norm1)                      [1024, 257, 768]     [1024, 257, 768]     1,536                True\n",
       "│    │    └─MemEffAttention (attn)                 [1024, 257, 768]     [1024, 257, 768]     2,362,368            True\n",
       "│    │    └─LayerScale (ls1)                       [1024, 257, 768]     [1024, 257, 768]     768                  True\n",
       "│    │    └─LayerNorm (norm2)                      [1024, 257, 768]     [1024, 257, 768]     1,536                True\n",
       "│    │    └─Mlp (mlp)                              [1024, 257, 768]     [1024, 257, 768]     4,722,432            True\n",
       "│    │    └─LayerScale (ls2)                       [1024, 257, 768]     [1024, 257, 768]     768                  True\n",
       "│    └─NestedTensorBlock (7)                       [1024, 257, 768]     [1024, 257, 768]     --                   True\n",
       "│    │    └─LayerNorm (norm1)                      [1024, 257, 768]     [1024, 257, 768]     1,536                True\n",
       "│    │    └─MemEffAttention (attn)                 [1024, 257, 768]     [1024, 257, 768]     2,362,368            True\n",
       "│    │    └─LayerScale (ls1)                       [1024, 257, 768]     [1024, 257, 768]     768                  True\n",
       "│    │    └─LayerNorm (norm2)                      [1024, 257, 768]     [1024, 257, 768]     1,536                True\n",
       "│    │    └─Mlp (mlp)                              [1024, 257, 768]     [1024, 257, 768]     4,722,432            True\n",
       "│    │    └─LayerScale (ls2)                       [1024, 257, 768]     [1024, 257, 768]     768                  True\n",
       "│    └─NestedTensorBlock (8)                       [1024, 257, 768]     [1024, 257, 768]     --                   True\n",
       "│    │    └─LayerNorm (norm1)                      [1024, 257, 768]     [1024, 257, 768]     1,536                True\n",
       "│    │    └─MemEffAttention (attn)                 [1024, 257, 768]     [1024, 257, 768]     2,362,368            True\n",
       "│    │    └─LayerScale (ls1)                       [1024, 257, 768]     [1024, 257, 768]     768                  True\n",
       "│    │    └─LayerNorm (norm2)                      [1024, 257, 768]     [1024, 257, 768]     1,536                True\n",
       "│    │    └─Mlp (mlp)                              [1024, 257, 768]     [1024, 257, 768]     4,722,432            True\n",
       "│    │    └─LayerScale (ls2)                       [1024, 257, 768]     [1024, 257, 768]     768                  True\n",
       "│    └─NestedTensorBlock (9)                       [1024, 257, 768]     [1024, 257, 768]     --                   True\n",
       "│    │    └─LayerNorm (norm1)                      [1024, 257, 768]     [1024, 257, 768]     1,536                True\n",
       "│    │    └─MemEffAttention (attn)                 [1024, 257, 768]     [1024, 257, 768]     2,362,368            True\n",
       "│    │    └─LayerScale (ls1)                       [1024, 257, 768]     [1024, 257, 768]     768                  True\n",
       "│    │    └─LayerNorm (norm2)                      [1024, 257, 768]     [1024, 257, 768]     1,536                True\n",
       "│    │    └─Mlp (mlp)                              [1024, 257, 768]     [1024, 257, 768]     4,722,432            True\n",
       "│    │    └─LayerScale (ls2)                       [1024, 257, 768]     [1024, 257, 768]     768                  True\n",
       "│    └─NestedTensorBlock (10)                      [1024, 257, 768]     [1024, 257, 768]     --                   True\n",
       "│    │    └─LayerNorm (norm1)                      [1024, 257, 768]     [1024, 257, 768]     1,536                True\n",
       "│    │    └─MemEffAttention (attn)                 [1024, 257, 768]     [1024, 257, 768]     2,362,368            True\n",
       "│    │    └─LayerScale (ls1)                       [1024, 257, 768]     [1024, 257, 768]     768                  True\n",
       "│    │    └─LayerNorm (norm2)                      [1024, 257, 768]     [1024, 257, 768]     1,536                True\n",
       "│    │    └─Mlp (mlp)                              [1024, 257, 768]     [1024, 257, 768]     4,722,432            True\n",
       "│    │    └─LayerScale (ls2)                       [1024, 257, 768]     [1024, 257, 768]     768                  True\n",
       "│    └─NestedTensorBlock (11)                      [1024, 257, 768]     [1024, 257, 768]     --                   True\n",
       "│    │    └─LayerNorm (norm1)                      [1024, 257, 768]     [1024, 257, 768]     1,536                True\n",
       "│    │    └─MemEffAttention (attn)                 [1024, 257, 768]     [1024, 257, 768]     2,362,368            True\n",
       "│    │    └─LayerScale (ls1)                       [1024, 257, 768]     [1024, 257, 768]     768                  True\n",
       "│    │    └─LayerNorm (norm2)                      [1024, 257, 768]     [1024, 257, 768]     1,536                True\n",
       "│    │    └─Mlp (mlp)                              [1024, 257, 768]     [1024, 257, 768]     4,722,432            True\n",
       "│    │    └─LayerScale (ls2)                       [1024, 257, 768]     [1024, 257, 768]     768                  True\n",
       "├─LayerNorm (norm)                                 [1024, 257, 768]     [1024, 257, 768]     1,536                True\n",
       "├─Identity (head)                                  [1024, 768]          [1024, 768]          --                   --\n",
       "==================================================================================================================================\n",
       "Total params: 86,580,480\n",
       "Trainable params: 86,580,480\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 205.68\n",
       "==================================================================================================================================\n",
       "Input size (MB): 616.56\n",
       "Forward/backward pass size (MB): 255464.57\n",
       "Params size (MB): 342.11\n",
       "Estimated Total Size (MB): 256423.24\n",
       "=================================================================================================================================="
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print a summary using torchinfo (uncomment for actual output)\n",
    "summary(model=dinov2_vitb14,\n",
    "        input_size=(1024,3,224,224),\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"]      \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LinearClassifier(nn.Module):\n",
    "    \"\"\"Linear layer to train on top of frozen features\"\"\"\n",
    "\n",
    "    def __init__(self, backbone, num_features=768, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.num_classes = num_classes\n",
    "        self.linear = nn.Linear(num_features, num_classes)\n",
    "        self.linear.weight.data.normal_(mean=0.0, std=0.01)\n",
    "        self.linear.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        output = self.linear(features)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearClassifier(backbone=dinov2_vitb14)\n",
    "for param in model.backbone.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=============================================================================================================================\n",
       "Layer (type (var_name))                       Input Shape          Output Shape         Param #              Trainable\n",
       "=============================================================================================================================\n",
       "LinearClassifier (LinearClassifier)           [1024, 3, 224, 224]  [1024, 2]            --                   Partial\n",
       "├─DinoVisionTransformer (backbone)            [1024, 3, 224, 224]  [1024, 768]          1,053,696            False\n",
       "│    └─PatchEmbed (patch_embed)               [1024, 3, 224, 224]  [1024, 256, 768]     --                   False\n",
       "│    │    └─Conv2d (proj)                     [1024, 3, 224, 224]  [1024, 768, 16, 16]  (452,352)            False\n",
       "│    │    └─Identity (norm)                   [1024, 256, 768]     [1024, 256, 768]     --                   --\n",
       "│    └─ModuleList (blocks)                    --                   --                   --                   False\n",
       "│    │    └─NestedTensorBlock (0)             [1024, 257, 768]     [1024, 257, 768]     (7,089,408)          False\n",
       "│    │    └─NestedTensorBlock (1)             [1024, 257, 768]     [1024, 257, 768]     (7,089,408)          False\n",
       "│    │    └─NestedTensorBlock (2)             [1024, 257, 768]     [1024, 257, 768]     (7,089,408)          False\n",
       "│    │    └─NestedTensorBlock (3)             [1024, 257, 768]     [1024, 257, 768]     (7,089,408)          False\n",
       "│    │    └─NestedTensorBlock (4)             [1024, 257, 768]     [1024, 257, 768]     (7,089,408)          False\n",
       "│    │    └─NestedTensorBlock (5)             [1024, 257, 768]     [1024, 257, 768]     (7,089,408)          False\n",
       "│    │    └─NestedTensorBlock (6)             [1024, 257, 768]     [1024, 257, 768]     (7,089,408)          False\n",
       "│    │    └─NestedTensorBlock (7)             [1024, 257, 768]     [1024, 257, 768]     (7,089,408)          False\n",
       "│    │    └─NestedTensorBlock (8)             [1024, 257, 768]     [1024, 257, 768]     (7,089,408)          False\n",
       "│    │    └─NestedTensorBlock (9)             [1024, 257, 768]     [1024, 257, 768]     (7,089,408)          False\n",
       "│    │    └─NestedTensorBlock (10)            [1024, 257, 768]     [1024, 257, 768]     (7,089,408)          False\n",
       "│    │    └─NestedTensorBlock (11)            [1024, 257, 768]     [1024, 257, 768]     (7,089,408)          False\n",
       "│    └─LayerNorm (norm)                       [1024, 257, 768]     [1024, 257, 768]     (1,536)              False\n",
       "│    └─Identity (head)                        [1024, 768]          [1024, 768]          --                   --\n",
       "├─Linear (linear)                             [1024, 768]          [1024, 2]            1,538                True\n",
       "=============================================================================================================================\n",
       "Total params: 86,582,018\n",
       "Trainable params: 1,538\n",
       "Non-trainable params: 86,580,480\n",
       "Total mult-adds (G): 205.68\n",
       "=============================================================================================================================\n",
       "Input size (MB): 616.56\n",
       "Forward/backward pass size (MB): 255464.59\n",
       "Params size (MB): 342.11\n",
       "Estimated Total Size (MB): 256423.26\n",
       "============================================================================================================================="
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model=model,\n",
    "        input_size=(1024,3,224,224),\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"]      \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[-6.9142e-04, -2.0539e-04, -4.6968e-02, -1.4960e-03, -2.0410e-02,\n",
      "           4.7294e-03,  2.6455e-03, -4.9413e-03, -4.9912e-03, -1.1096e-03,\n",
      "           2.4690e-03,  7.5610e-03,  7.1985e-03, -3.0513e-04, -3.3493e-03,\n",
      "          -3.6020e-04,  1.2004e-02,  1.3525e-03, -1.4590e-02, -8.5887e-03,\n",
      "          -7.0904e-04,  2.0706e-04, -1.8931e-03, -1.6581e-03, -1.7453e-03,\n",
      "           1.5737e-03, -3.3249e-03, -3.5086e-04,  4.2582e-03, -6.0191e-03,\n",
      "          -1.9831e-03,  3.4883e-04, -5.7279e-03,  3.4933e-02,  4.3051e-03,\n",
      "           2.9389e-03, -9.7192e-04,  8.2199e-03,  3.6512e-03, -1.9048e-03,\n",
      "           2.6950e-03, -4.7313e-02, -1.9887e-02, -7.3382e-04,  7.6832e-03,\n",
      "          -2.8708e-02, -1.3646e-03, -1.8689e-04, -1.7721e-03, -4.0460e-03,\n",
      "          -3.8712e-04,  8.3599e-03,  1.1057e-03, -2.7552e-03,  1.3896e-02,\n",
      "          -2.5532e-03,  1.4683e-02, -3.9614e-03,  9.5134e-04,  3.6621e-03,\n",
      "           3.0747e-04, -2.3839e-03, -1.7077e-03,  2.7554e-03,  2.9600e-03,\n",
      "           6.3919e-04,  2.9736e-05, -8.8576e-03, -1.7528e-05, -9.9480e-02,\n",
      "           6.9751e-04, -1.3330e-02, -6.1896e-03,  6.0750e-04,  2.6725e-03,\n",
      "           6.5834e-03,  3.5721e-02,  5.0348e-03,  1.3250e-02, -7.8895e-03,\n",
      "           7.8154e-04, -1.7075e-02, -9.1432e-04, -2.5645e-03, -2.6257e-04,\n",
      "           2.5842e-02, -7.8633e-03,  2.1113e-03,  5.8288e-03, -4.0973e-04,\n",
      "          -2.0175e-03, -4.0378e-02, -5.4853e-03, -9.5237e-04, -1.2127e-02,\n",
      "          -2.6705e-03,  9.3575e-03,  7.3882e-04,  2.7126e-04, -3.3116e-04,\n",
      "          -4.0404e-03,  1.0292e-03, -1.0962e-02, -7.9162e-03,  5.0789e-03,\n",
      "           4.3364e-02, -2.2088e-03, -1.7565e-03,  8.3689e-04,  8.0990e-03,\n",
      "          -8.2258e-03,  4.3192e-04, -6.2071e-04,  1.7155e-02,  1.9537e-02,\n",
      "           4.1364e-03,  9.1184e-03,  6.3641e-03,  9.6773e-03, -5.9772e-03,\n",
      "          -1.9036e-03, -2.3713e-03, -6.3699e-03, -1.6664e-03, -4.5654e-04,\n",
      "           5.0457e-03,  7.8852e-03, -2.2665e-03, -9.7837e-04, -5.0671e-03,\n",
      "          -2.7863e-04, -6.2242e-03, -9.8279e-04, -3.8433e-03,  2.3165e-02,\n",
      "           1.5947e-03, -2.8903e-03, -3.3554e-03, -9.6689e-03,  2.3904e-03,\n",
      "          -2.3063e-02, -1.7633e-02,  2.1082e-02, -3.5099e-03,  1.0736e-02,\n",
      "           1.9963e-03,  4.5194e-03, -5.4116e-04,  2.7463e-03, -7.0032e-03,\n",
      "          -2.7960e-02,  1.2307e-02, -5.5295e-04, -5.1515e-03,  3.5977e-03,\n",
      "          -3.3837e-03, -1.5137e-02,  1.3421e-02,  4.4545e-03, -1.4067e-03,\n",
      "          -9.7089e-04,  3.9150e-03,  1.7974e-02,  3.1601e-03, -1.5749e-03,\n",
      "          -2.7065e-02, -4.0961e-02, -1.2035e-02, -2.3037e-03,  5.5220e-04,\n",
      "          -2.1144e-02,  5.8146e-03,  7.6740e-03, -8.4147e-04,  1.0512e-03,\n",
      "           3.0779e-02, -2.1628e-04,  1.8324e-03, -2.7554e-03, -2.6647e-03,\n",
      "          -8.8163e-03,  8.6852e-03,  3.3477e-03, -1.2601e-03, -1.9949e-03,\n",
      "           1.2064e-02, -5.3506e-03,  6.8403e-03,  2.0206e-03, -9.1380e-03,\n",
      "           2.2131e-03,  1.2461e-03, -6.8137e-05,  6.7122e-04,  5.9609e-03,\n",
      "           2.7783e-03,  4.8442e-02, -2.0791e-02, -2.9070e-03,  8.5606e-03,\n",
      "           4.1936e-03, -5.4919e-03,  3.6339e-03,  1.9192e-03, -2.6383e-03,\n",
      "           2.6909e-03,  3.3121e-02,  2.2051e-03, -3.7046e-03,  4.4818e-03,\n",
      "           6.8856e-03, -1.6628e-02, -1.5929e-02,  8.0418e-03, -1.1650e-03,\n",
      "           1.6368e-03, -1.7888e-03, -9.3590e-03,  4.2569e-03, -6.7730e-04,\n",
      "           1.3145e-02, -1.4111e-02, -7.0879e-03,  1.7917e-03, -2.1328e-03,\n",
      "           7.6852e-04, -3.1128e-04, -1.7372e-02,  1.5815e-05,  1.3745e-02,\n",
      "          -1.2679e-04, -6.4609e-03, -9.1253e-04,  3.7675e-03,  1.9369e-03,\n",
      "           2.8731e-02, -8.9808e-03,  1.8228e-02,  5.3838e-03,  7.6011e-03,\n",
      "          -2.5345e-03, -4.0378e-03, -3.4093e-02, -7.3530e-05, -7.2307e-04,\n",
      "           1.9166e-03, -4.7617e-04, -5.1698e-04, -1.7261e-03,  1.4827e-03,\n",
      "          -5.3541e-03, -3.0445e-03, -2.5619e-02, -3.5412e-03, -1.8466e-02,\n",
      "          -1.0386e-02,  1.8112e-04, -2.4965e-03,  4.1264e-04, -5.1434e-03,\n",
      "          -6.7881e-03, -5.6914e-03,  2.9242e-04,  4.2101e-03, -2.5850e-02,\n",
      "          -8.3915e-04, -5.7859e-03,  6.4007e-03, -1.4391e-02, -4.0646e-03,\n",
      "           6.2434e-03,  3.3521e-03,  5.2703e-03,  8.0055e-03,  1.9180e-02,\n",
      "           5.5000e-03, -1.3477e-03, -4.4125e-03, -2.7021e-04, -1.5935e-04,\n",
      "           1.1251e-02, -2.2345e-03, -3.0933e-03,  3.7453e-03,  6.8332e-03,\n",
      "           9.0136e-04, -9.6569e-04,  6.8917e-03,  7.0121e-03, -1.1574e-02,\n",
      "           1.8792e-02,  7.3847e-02,  2.7942e-03,  1.3262e-03, -1.2832e-02,\n",
      "          -5.9202e-03,  5.8564e-03, -3.9118e-04,  1.1301e-02, -9.9142e-03,\n",
      "          -4.3575e-03, -2.7955e-03,  1.7741e-03, -9.3865e-04,  6.4755e-04,\n",
      "          -8.5172e-04, -4.3751e-03, -2.4987e-03, -3.9769e-02, -1.9375e-03,\n",
      "           8.2094e-04, -6.2816e-04,  8.6478e-03, -4.1294e-03, -4.0879e-03,\n",
      "          -2.2447e-03,  6.3143e-03,  1.4944e-02, -1.4791e-04, -1.9769e-03,\n",
      "           3.5696e-02,  1.5033e-02, -7.1289e-03, -3.6699e-03, -3.5124e-03,\n",
      "           5.8396e-03, -7.9475e-03,  2.1330e-04,  8.8644e-04,  1.6026e-03,\n",
      "          -1.1693e-02, -6.7311e-03,  8.8395e-03,  2.4958e-03, -1.9431e-02,\n",
      "           7.2783e-04, -1.0571e-02,  4.7288e-02,  7.9129e-04, -1.6335e-02,\n",
      "          -4.5010e-03, -9.5187e-03, -6.2751e-03, -2.3750e-03,  7.8936e-03,\n",
      "           1.3981e-03,  4.9661e-04,  2.9061e-03,  6.3141e-03, -1.0442e-02,\n",
      "           2.5026e-03,  8.7619e-04, -4.8590e-03, -2.1093e-04, -3.6313e-03,\n",
      "           1.5313e-03,  1.2137e-03, -1.6315e-03,  1.2405e-03, -1.2578e-03,\n",
      "          -1.9566e-02,  2.2374e-03, -1.3614e-02,  1.1526e-02, -2.2023e-03,\n",
      "           2.6595e-03,  6.0413e-06, -4.1683e-03,  3.9147e-03, -6.6045e-03,\n",
      "          -1.7045e-03, -6.5630e-03, -5.1516e-05, -6.9670e-03,  1.0076e-03,\n",
      "           2.6348e-03, -2.8985e-02,  1.0183e-03, -7.0555e-04, -3.6251e-03,\n",
      "           3.6363e-03, -2.9929e-03, -1.8646e-04,  1.2165e-03, -7.7792e-03,\n",
      "          -3.2400e-03,  1.0015e-02,  1.1676e-03,  4.8544e-03,  8.1868e-03,\n",
      "          -7.6716e-03, -2.1741e-03,  3.2288e-02, -1.5238e-03, -7.8092e-03,\n",
      "           2.6046e-04, -1.3477e-02, -1.4532e-04, -5.9686e-03,  3.9466e-03,\n",
      "           9.5322e-03, -1.2113e-02,  6.4046e-03, -1.0882e-02, -5.8089e-05,\n",
      "          -2.0439e-03,  1.5855e-02, -2.8226e-04, -3.3618e-02,  8.3494e-03,\n",
      "          -1.5095e-03, -1.0833e-02,  8.4694e-03,  1.8181e-02,  1.0930e-02,\n",
      "           1.2223e-01,  1.0445e-03, -1.6313e-04, -2.0064e-02,  5.6378e-02,\n",
      "           1.1407e-02,  1.4537e-02, -1.4722e-03,  9.8319e-03,  2.7342e-03,\n",
      "          -1.4818e-03, -1.7192e-03,  1.8990e-05,  3.7080e-03, -5.7490e-03,\n",
      "          -6.9555e-03, -6.8912e-03,  1.1545e-03,  9.2321e-03, -3.5696e-03,\n",
      "          -1.2519e-02, -5.6130e-04,  1.3159e-03, -1.6761e-02,  3.2672e-03,\n",
      "          -6.7385e-03,  3.9767e-04,  7.6499e-04, -2.3299e-03,  2.5520e-03,\n",
      "          -2.1958e-03,  1.8599e-02, -3.4244e-03, -1.5240e-02, -2.8121e-03,\n",
      "          -2.7448e-03, -1.6996e-02,  3.6577e-03, -9.7088e-03,  5.7728e-02,\n",
      "          -4.7559e-03, -7.5824e-03, -8.5942e-03, -1.9673e-03,  4.4505e-03,\n",
      "           3.3987e-02,  3.3903e-02, -4.2073e-03, -1.3705e-02, -2.9193e-03,\n",
      "           1.5046e-02, -8.7551e-04,  1.5230e-03,  6.8406e-02,  4.5112e-04,\n",
      "          -1.6597e-02,  9.6955e-04,  5.8333e-03, -1.3687e-04,  2.6734e-03,\n",
      "          -1.7776e-02,  2.5790e-03, -7.8984e-03, -1.1036e-03, -8.6867e-03,\n",
      "           6.1607e-03,  6.6218e-03,  1.2665e-02,  2.0914e-03, -1.1475e-02,\n",
      "          -3.3564e-02, -1.1282e-02, -2.1595e-03, -2.1978e-04,  8.4031e-05,\n",
      "           5.9037e-03, -1.0158e-01, -1.7317e-03, -4.0069e-03, -2.1470e-03,\n",
      "          -1.9151e-02,  8.2512e-03,  9.5994e-02,  3.6766e-03,  2.3271e-02,\n",
      "           7.5014e-03, -8.0520e-03,  2.5840e-03, -3.8415e-03, -9.3854e-04,\n",
      "          -3.8803e-03,  3.3160e-03,  1.3149e-03, -4.6106e-03,  2.7332e-03,\n",
      "          -2.9660e-03, -1.5518e-02, -2.0260e-03, -2.4439e-03,  4.6040e-03,\n",
      "           5.4111e-03, -2.3689e-03, -5.3347e-03,  3.0415e-03,  8.8139e-05,\n",
      "          -4.9361e-02, -6.2360e-03,  4.3348e-03,  7.1709e-04, -1.1165e-03,\n",
      "           2.8119e-03, -3.0449e-03, -1.8724e-02, -4.6449e-03,  7.2853e-04,\n",
      "           1.6795e-03,  1.4342e-02,  2.5313e-03,  5.2889e-03,  2.4826e-03,\n",
      "           1.3712e-03, -1.4282e-03,  2.2293e-03, -1.4178e-02, -1.3558e-03,\n",
      "           7.8251e-03,  4.5015e-03,  6.7904e-03, -4.7418e-03, -1.1900e-02,\n",
      "           2.6463e-03, -3.1873e-03,  6.3225e-04, -5.1941e-03, -1.8142e-03,\n",
      "           1.3581e-02,  1.4383e-02,  2.2076e-02, -1.2113e-02, -1.1716e-03,\n",
      "          -2.9452e-03, -1.6513e-03, -6.6028e-04,  2.0874e-03, -1.1455e-03,\n",
      "          -1.9706e-02, -1.8358e-03, -1.0752e-02,  3.2165e-03,  5.7565e-03,\n",
      "           3.4230e-03, -4.4993e-03, -2.0848e-02,  3.8444e-02,  7.7683e-03,\n",
      "          -8.0977e-03, -2.1381e-03, -2.9113e-03,  1.2745e-03,  4.1636e-04,\n",
      "           1.0729e-02, -5.8648e-03, -3.1573e-03, -1.8260e-03,  2.5609e-03,\n",
      "           1.0305e-03,  2.3730e-04,  8.8099e-04, -1.8698e-03,  5.2023e-04,\n",
      "          -8.4527e-03,  2.7271e-03,  9.6368e-03,  4.6615e-03,  1.4628e-02,\n",
      "           1.0973e-02, -2.0214e-02, -9.7395e-05, -1.4660e-02, -3.0912e-03,\n",
      "          -2.9707e-03,  1.5966e-03,  2.8878e-03,  1.6290e-03,  1.8066e-03,\n",
      "           4.6752e-03,  9.7292e-04,  2.1142e-03,  1.0142e-02, -1.0998e-02,\n",
      "          -1.0911e-02, -5.0496e-04,  2.4211e-03, -7.6438e-03,  5.1610e-03,\n",
      "          -2.4121e-03,  3.6602e-04,  4.4389e-03,  1.4062e-03,  9.1884e-03,\n",
      "          -3.0517e-03, -2.0251e-03,  1.4707e-02,  2.2004e-03,  7.0563e-04,\n",
      "          -4.6458e-03, -1.8824e-02, -3.7676e-03, -1.6966e-04,  4.4607e-03,\n",
      "          -5.7389e-03, -1.9757e-02,  3.8462e-03, -3.0497e-03,  6.8848e-03,\n",
      "          -1.5443e-03,  2.2698e-03, -2.7822e-04, -3.5757e-03, -5.9757e-03,\n",
      "           2.6313e-02, -1.1521e-06, -5.0765e-03,  2.4773e-03, -1.4929e-02,\n",
      "           8.1408e-04,  5.3171e-04,  1.0641e-02, -1.6763e-02,  4.0987e-03,\n",
      "          -4.2517e-03, -3.2439e-05, -1.0132e-03, -2.0964e-03, -5.2620e-03,\n",
      "           2.0337e-03, -6.1210e-03,  2.9525e-04, -2.3450e-04, -6.0312e-03,\n",
      "          -2.1916e-03,  3.0266e-03, -7.6882e-04,  1.5419e-04,  5.7188e-04,\n",
      "           1.2172e-02, -4.8183e-02, -2.2276e-03, -8.4232e-03,  3.0409e-03,\n",
      "           1.6446e-02, -7.3993e-03, -1.5623e-02, -7.6967e-03, -1.5241e-03,\n",
      "           2.8429e-03,  5.5379e-03, -1.1298e-03,  2.0695e-03, -1.0170e-02,\n",
      "          -6.0071e-03, -3.3950e-04,  5.5615e-04, -5.9190e-03,  2.6403e-03,\n",
      "           3.7327e-02, -5.5061e-03, -1.1194e-03, -1.2117e-02,  2.4951e-02,\n",
      "          -1.8798e-02,  1.2151e-04,  7.4267e-04,  2.2009e-03, -1.1332e-03,\n",
      "          -5.4717e-04, -1.0754e-02, -9.9564e-03, -4.3429e-04, -2.8698e-03,\n",
      "          -3.7392e-03,  8.9735e-02,  3.0166e-03, -1.6486e-03, -9.9023e-03,\n",
      "          -5.8886e-04, -6.9942e-03,  7.9101e-04, -1.2491e-02,  3.5885e-03,\n",
      "           6.1476e-02,  2.1783e-02,  1.2274e-02, -4.2751e-02,  2.5297e-03,\n",
      "           3.8512e-03, -3.9055e-03, -1.8133e-03,  3.2818e-03, -6.7237e-03,\n",
      "           4.8217e-03, -3.0639e-03,  6.3157e-04,  3.9786e-03,  3.3271e-03,\n",
      "           8.3519e-03,  7.2032e-06, -9.8246e-03, -1.7326e-02, -1.1415e-03,\n",
      "          -1.6067e-03, -3.7319e-03, -5.7133e-03, -8.8477e-03,  1.6201e-02,\n",
      "           1.3365e-02,  3.1511e-02,  2.6418e-02, -1.6601e-03,  4.5850e-03,\n",
      "          -1.0531e-02, -1.1716e-03,  9.9282e-03, -1.5506e-03,  2.5167e-04,\n",
      "           4.7908e-03,  9.5451e-03, -2.3802e-03, -4.4147e-03,  2.1545e-02,\n",
      "          -4.9717e-03, -6.5619e-03,  6.9631e-03,  8.3479e-04, -4.2847e-03,\n",
      "           3.2307e-03, -7.9498e-03,  1.8521e-02, -4.2198e-03,  1.6408e-03,\n",
      "          -2.2132e-04, -9.4947e-04,  1.5255e-03,  7.2051e-04,  7.2600e-03,\n",
      "          -6.0036e-03,  2.0380e-02,  1.9925e-03,  7.5331e-03, -1.0696e-02,\n",
      "           7.1195e-03,  5.5863e-03,  9.0711e-04]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for parameters in model.backbone.parameters():\n",
    "    print(parameters)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dinov2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
